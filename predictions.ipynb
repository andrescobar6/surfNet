{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import ccxt\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import google.cloud\n",
    "import tweepy as tw\n",
    "from pandas_gbq import gbq\n",
    "import matplotlib.cm as cm\n",
    "from boruta import BorutaPy\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import bigquery\n",
    "from sklearn import preprocessing\n",
    "from requests import Request, Session\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "\n",
    "#_____\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCIÓN QUE LEE EL ARCHIVO CONFIG\n",
    "def get_config(category, key):\n",
    "    \n",
    "    global config\n",
    "    return config[category][key]\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE GENERA CONEXIÓN CON ARCHIVO CONFIG\n",
    "def updateConfig(config_name):\n",
    "    \n",
    "    global config\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.sections()\n",
    "    config.read(config_name)\n",
    "    \n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE GRAFICA POR COLORES SEGUN COMPRAS Y NO COMRPAS\n",
    "def plot_colourline(x,y,c):\n",
    "    c = c\n",
    "    ax = plt.gca()\n",
    "    for i in np.arange(len(x)-1):\n",
    "        ax.plot([x[i],x[i+1]], [y[i],y[i+1]], c=c[i])\n",
    "    return\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE ESCRIBE PICKLE DE VARIABLE\n",
    "def writePickleVariable(variable,variable_name):\n",
    "    pickle_out = open(variable_name+\".pickle\",\"wb\")\n",
    "    pickle.dump(variable, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "#_____\n",
    "    \n",
    "#FUNCIÓN QUE LEE PICKLE DE VARIABLE\n",
    "def readPickleVariable(variable_name):    \n",
    "    return pickle.load(open(variable_name+\".pickle\",\"rb\"))\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE REDONDEA FLOAT HACIA ABAJO SEGÚN DECIMALES\n",
    "def round_decimals_down(number:float, decimals:int=2):\n",
    "    \"\"\"\n",
    "    Returns a value rounded down to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more\")\n",
    "    elif decimals == 0:\n",
    "        return math.floor(number)\n",
    "\n",
    "    factor = 10 ** decimals\n",
    "    return math.floor(number * factor) / factor\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE REDONDEA FLOAT HACIA ARRIBA SEGÚN DECIMALES\n",
    "def round_decimals_up(number:float, decimals:int=2):\n",
    "    \"\"\"\n",
    "    Returns a value rounded down to a specific number of decimal places.\n",
    "    \"\"\"\n",
    "    if not isinstance(decimals, int):\n",
    "        raise TypeError(\"decimal places must be an integer\")\n",
    "    elif decimals < 0:\n",
    "        raise ValueError(\"decimal places has to be 0 or more\")\n",
    "    elif decimals == 0:\n",
    "        return math.ceil(number)\n",
    "\n",
    "    factor = 10 ** decimals\n",
    "    return math.ceil(number * factor) / factor\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DESCARGA BASE DE DATOS DE MERCADO DE BIGQUERY\n",
    "def downloadDataBaseBigQuery(market_model,time,rows=None):\n",
    "    \n",
    "    #_____GOOGLE CLOUD CONECTION\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/ubuntu/SurfNet/predictionModels/PREDICTION/bigQueryAccess.json\"\n",
    "\n",
    "    #_____SI NO SE DA UN NÚMERO ESPECÍFICO DE FILAS POR PARÁMETRO\n",
    "    if rows==None:\n",
    "\n",
    "        #_____DESCARGAR BASE DE DATOS\n",
    "        candlesDataBase_BigQuery=gbq.read_gbq(\"SELECT * FROM [dogwood-terra-308100:surfNet.\"+market_model.replace(\"/\",\"_\")+\"] ORDER BY TIME DESC\",project_id=\"dogwood-terra-308100\",dialect=\"legacy\").sort_values(by=\"TIME\")\n",
    "        candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #_____DESCARGAR BASE DE DATOS\n",
    "        candlesDataBase_BigQuery=gbq.read_gbq(\"SELECT * FROM [dogwood-terra-308100:surfNet.\"+market_model.replace(\"/\",\"_\")+\"] ORDER BY TIME DESC LIMIT \"+str(rows),project_id=\"dogwood-terra-308100\",dialect=\"legacy\").sort_values(by=\"TIME\")\n",
    "        candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    candlesDataBase_BigQuery.OPEN=candlesDataBase_BigQuery.OPEN.astype(float)\n",
    "    candlesDataBase_BigQuery.HIGH=candlesDataBase_BigQuery.HIGH.astype(float)\n",
    "    candlesDataBase_BigQuery.LOW=candlesDataBase_BigQuery.LOW.astype(float)\n",
    "    candlesDataBase_BigQuery.CLOSE=candlesDataBase_BigQuery.CLOSE.astype(float)\n",
    "    candlesDataBase_BigQuery.VOLUME=candlesDataBase_BigQuery.VOLUME.astype(float)\n",
    "\n",
    "    #_____REESTRUCTURAR BASE DE DATOS\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    if time != \"1\":\n",
    "\n",
    "        #_____UPDATE CONFIG PREDICTION\n",
    "        updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "        #_____CREAR COLUMNAS\n",
    "        candlesDataBase_BigQuery_copy=candlesDataBase_BigQuery.copy()\n",
    "        candlesDataBase_BigQuery_copy=candlesDataBase_BigQuery_copy.set_index('TIME')\n",
    "        candlesDataBase_BigQuery_copy_total=candlesDataBase_BigQuery_copy[[\"ID\"]].groupby(pd.Grouper(freq=time+'min', offset=time+'min')).min()\n",
    "        candlesDataBase_BigQuery_copy_total=candlesDataBase_BigQuery_copy_total.join(candlesDataBase_BigQuery_copy[[\"VOLUME\"]].groupby(pd.Grouper(freq=time+'min', offset=time+'min')).sum())\n",
    "        candlesDataBase_BigQuery_copy_total=candlesDataBase_BigQuery_copy_total.join(candlesDataBase_BigQuery_copy[[\"LOW\"]].groupby(pd.Grouper(freq=time+'min', offset=time+'min')).min())\n",
    "        candlesDataBase_BigQuery_copy_total=candlesDataBase_BigQuery_copy_total.join(candlesDataBase_BigQuery_copy[[\"HIGH\"]].groupby(pd.Grouper(freq=time+'min', offset=time+'min')).max())\n",
    "\n",
    "        #_____AGREAR DATOS\n",
    "        for i in list(candlesDataBase_BigQuery_copy_total.index.values):\n",
    "            try:\n",
    "                candlesDataBase_BigQuery_copy_total.at[i,\"OPEN\"]=candlesDataBase_BigQuery.loc[candlesDataBase_BigQuery.TIME==i][\"OPEN\"].values[0]\n",
    "                final_time = i + np.timedelta64(int(time),'m')\n",
    "                if final_time in list(candlesDataBase_BigQuery_copy_total.index.values):\n",
    "                    candlesDataBase_BigQuery_copy_total.at[i,\"CLOSE\"]=candlesDataBase_BigQuery.loc[candlesDataBase_BigQuery.TIME==final_time][\"OPEN\"].values[0]\n",
    "                else:\n",
    "                    candlesDataBase_BigQuery_copy_total.at[i,\"CLOSE\"]=candlesDataBase_BigQuery.at[len(candlesDataBase_BigQuery)-1,\"CLOSE\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        #_____RESET INDEX\n",
    "        candlesDataBase_BigQuery_copy_total.reset_index(level=['TIME'],inplace=True)\n",
    "\n",
    "        #_____COLUMNA MARKET\n",
    "        for i in range(0,len(candlesDataBase_BigQuery_copy_total)):\n",
    "            candlesDataBase_BigQuery_copy_total.at[i,\"MARKET\"]=candlesDataBase_BigQuery.MARKET.values[0]\n",
    "\n",
    "        #_____ORDENAR COLUMNAS\n",
    "        column_names = list(candlesDataBase_BigQuery.columns)\n",
    "        candlesDataBase_BigQuery_copy_total = candlesDataBase_BigQuery_copy_total.reindex(columns=column_names)\n",
    "        candlesDataBase_BigQuery_copy_total.dropna(inplace=True)\n",
    "        candlesDataBase_BigQuery_copy_total.reset_index(inplace=True,drop=True)\n",
    "        candlesDataBase_BigQuery=candlesDataBase_BigQuery_copy_total.copy()\n",
    "\n",
    "    #_____RETURN\n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DEVUELVE EL RSI DADO UN PERIODO DETERMINADO\n",
    "def RSI(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global config\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    series=pd.Series(list(candlesDataBase_BigQuery.CLOSE.values))\n",
    "    period=int(get_config(\"PARAMETERS\",\"RSI_LEN\"))\n",
    "    \n",
    "    delta = series.diff().dropna()\n",
    "    u = delta * 0\n",
    "    d = u.copy()\n",
    "    u[delta > 0] = delta[delta > 0]\n",
    "    d[delta < 0] = -delta[delta < 0]\n",
    "    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(period-1)])\n",
    "    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(period-1)])\n",
    "    rs = pd.DataFrame.ewm(u, com=period-1, adjust=False).mean() / \\\n",
    "         pd.DataFrame.ewm(d, com=period-1, adjust=False).mean()\n",
    "    \n",
    "    rsi=100 - 100 / (1 + rs)\n",
    "    \n",
    "    for i in list(rsi.index.values):\n",
    "        candlesDataBase_BigQuery.at[i,\"RSI\"]=rsi[i]\n",
    "                                        \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DEVUELVE EL RSI DADO UN PERIODO DETERMINADO\n",
    "def RSI_GENERAL(series, period):\n",
    "    delta = series.diff().dropna()\n",
    "    u = delta * 0\n",
    "    d = u.copy()\n",
    "    u[delta > 0] = delta[delta > 0]\n",
    "    d[delta < 0] = -delta[delta < 0]\n",
    "    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(period-1)])\n",
    "    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(period-1)])\n",
    "    rs = pd.DataFrame.ewm(u, com=period-1, adjust=False).mean() / \\\n",
    "         pd.DataFrame.ewm(d, com=period-1, adjust=False).mean()\n",
    "    return 100 - 100 / (1 + rs)\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE CALCULA MACD\n",
    "def MACD(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global config\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    close_data_df=candlesDataBase_BigQuery.copy()\n",
    "    close_data_df.CLOSE=close_data_df.CLOSE.astype(float)\n",
    "    close_data_df=close_data_df[[\"CLOSE\"]]\n",
    "    \n",
    "    #_____CREAR VARIABLE MACD\n",
    "    macd_12 = close_data_df.ewm(span=int(get_config(\"PARAMETERS\",\"MACD_SHORT\")), adjust=False).mean()\n",
    "    macd_26 = close_data_df.ewm(span=int(get_config(\"PARAMETERS\",\"MACD_LONG\")), adjust=False).mean()\n",
    "    macd = macd_12 - macd_26\n",
    "    macd_9 = macd.ewm(span=int(get_config(\"PARAMETERS\",\"MACD_TRIGGER\")), adjust=False).mean()\n",
    "\n",
    "    #_____AGREGAR VARIABLES A LA BASE DE DATOS\n",
    "    candlesDataBase_BigQuery[\"MACD\"]=macd\n",
    "    #candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_SHORT\")]=macd_12\n",
    "    #candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_LONG\")]=macd_26\n",
    "    candlesDataBase_BigQuery[\"MACD_TRIGGER\"]=macd_9\n",
    "    \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE BRINDA LOS NIVELES DE FIBONACCI\n",
    "def fibonacciLevels(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global config\n",
    "    global fractal\n",
    "    \n",
    "    #_____UPDATE CONFIG PREDICTION\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    #_____ACTUALIZAR PARÁMETROS\n",
    "    FIBONACCI_PERIOD=int(int(get_config(\"PARAMETERS\",\"FIBONACCI_PERIOD\"))/int(fractal))\n",
    "\n",
    "    #_____RECORRER DATOS DE TEST\n",
    "    for i in range(FIBONACCI_PERIOD,len(candlesDataBase_BigQuery)):\n",
    "        \n",
    "        #_____FIBONACCI\n",
    "        lows=list(candlesDataBase_BigQuery[i-FIBONACCI_PERIOD:i].LOW.values)\n",
    "        highs=list(candlesDataBase_BigQuery[i-FIBONACCI_PERIOD:i].HIGH.values)\n",
    "        Price_Min = min(lows)\n",
    "        Price_Max = max(highs)\n",
    "        Diff = Price_Max-Price_Min\n",
    "        level1 = Price_Max - 0.236 * Diff\n",
    "        level2 = Price_Max - 0.382 * Diff\n",
    "        level3 = Price_Max - 0.618 * Diff\n",
    "        level4 = Price_Max - 0.786 * Diff\n",
    "        \n",
    "        candlesDataBase_BigQuery.at[i,\"FIBO_LEVEL_1\"]=level1\n",
    "        candlesDataBase_BigQuery.at[i,\"FIBO_LEVEL_2\"]=level2\n",
    "        candlesDataBase_BigQuery.at[i,\"FIBO_LEVEL_3\"]=level3\n",
    "        candlesDataBase_BigQuery.at[i,\"FIBO_LEVEL_4\"]=level4\n",
    "            \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE CREA VARIABLES ENCONTRADAS EN ARTÍCULOS\n",
    "def newVariables(candlesDataBase_BigQuery):\n",
    "    \n",
    "    #_____VARIABLES NUEVAS\n",
    "    candlesDataBase_BigQuery['SMA5_VOL'] = candlesDataBase_BigQuery.groupby('MARKET')['VOLUME'].transform(lambda x: x.rolling(window = 5).mean())\n",
    "    candlesDataBase_BigQuery['SMA15_VOL'] = candlesDataBase_BigQuery.groupby('MARKET')['VOLUME'].transform(lambda x: x.rolling(window = 15).mean())\n",
    "    candlesDataBase_BigQuery['SMA_VOL_RATIO'] = candlesDataBase_BigQuery['SMA5_VOL']/candlesDataBase_BigQuery['SMA15_VOL']\n",
    "\n",
    "    #_____VARIABLES NUEVAS\n",
    "    candlesDataBase_BigQuery['LOWEST_5D'] = candlesDataBase_BigQuery.groupby('MARKET')['LOW'].transform(lambda x: x.rolling(window = 5).min())\n",
    "    candlesDataBase_BigQuery['HIGH_5D'] = candlesDataBase_BigQuery.groupby('MARKET')['HIGH'].transform(lambda x: x.rolling(window = 5).max())\n",
    "    candlesDataBase_BigQuery['LOWEST_15D'] = candlesDataBase_BigQuery.groupby('MARKET')['LOW'].transform(lambda x: x.rolling(window = 15).min())\n",
    "    candlesDataBase_BigQuery['HIGH_15D'] = candlesDataBase_BigQuery.groupby('MARKET')['HIGH'].transform(lambda x: x.rolling(window = 15).max())\n",
    "    candlesDataBase_BigQuery['STOCHASTIC_5'] = ((candlesDataBase_BigQuery['CLOSE'] - candlesDataBase_BigQuery['LOWEST_5D'])/(candlesDataBase_BigQuery['HIGH_5D'] - candlesDataBase_BigQuery['LOWEST_5D']))*100\n",
    "    candlesDataBase_BigQuery['STOCHASTIC_15'] = ((candlesDataBase_BigQuery['CLOSE'] - candlesDataBase_BigQuery['LOWEST_15D'])/(candlesDataBase_BigQuery['HIGH_15D'] - candlesDataBase_BigQuery['LOWEST_15D']))*100\n",
    "    candlesDataBase_BigQuery['STOCHASTIC_D_5'] = candlesDataBase_BigQuery['STOCHASTIC_5'].rolling(window = 5).mean()\n",
    "    candlesDataBase_BigQuery['STOCHASTIC_D_15'] = candlesDataBase_BigQuery['STOCHASTIC_5'].rolling(window = 15).mean()\n",
    "    candlesDataBase_BigQuery['STOCHASTIC_RATIO'] = candlesDataBase_BigQuery['STOCHASTIC_D_5']/candlesDataBase_BigQuery['STOCHASTIC_D_15']\n",
    "    \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN SUPERTREND\n",
    "def superTrend(df, atr_period, multiplier):\n",
    "    \n",
    "    high = df['HIGH']\n",
    "    low = df['LOW']\n",
    "    close = df['CLOSE']\n",
    "    \n",
    "    # calculate ATR\n",
    "    price_diffs = [high - low, \n",
    "                   high - close.shift(), \n",
    "                   close.shift() - low]\n",
    "    true_range = pd.concat(price_diffs, axis=1)\n",
    "    true_range = true_range.abs().max(axis=1)\n",
    "    # default ATR calculation in supertrend indicator\n",
    "    atr = true_range.ewm(alpha=1/atr_period,min_periods=atr_period).mean() \n",
    "    # df['atr'] = df['tr'].rolling(atr_period).mean()\n",
    "    \n",
    "    # HL2 is simply the average of high and low prices\n",
    "    hl2 = (high + low) / 2\n",
    "    # upperband and lowerband calculation\n",
    "    # notice that final bands are set to be equal to the respective bands\n",
    "    final_upperband = upperband = hl2 + (multiplier * atr)\n",
    "    final_lowerband = lowerband = hl2 - (multiplier * atr)\n",
    "    \n",
    "    # initialize Supertrend column to True\n",
    "    supertrend = [True] * len(df)\n",
    "    \n",
    "    for i in range(1, len(df.index)):\n",
    "        curr, prev = i, i-1\n",
    "        \n",
    "        # if current close price crosses above upperband\n",
    "        if close[curr] > final_upperband[prev]:\n",
    "            supertrend[curr] = True\n",
    "        # if current close price crosses below lowerband\n",
    "        elif close[curr] < final_lowerband[prev]:\n",
    "            supertrend[curr] = False\n",
    "        # else, the trend continues\n",
    "        else:\n",
    "            supertrend[curr] = supertrend[prev]\n",
    "            \n",
    "            # adjustment to the final bands\n",
    "            if supertrend[curr] == True and final_lowerband[curr] < final_lowerband[prev]:\n",
    "                final_lowerband[curr] = final_lowerband[prev]\n",
    "            if supertrend[curr] == False and final_upperband[curr] > final_upperband[prev]:\n",
    "                final_upperband[curr] = final_upperband[prev]\n",
    "\n",
    "        # to remove bands according to the trend direction\n",
    "        if supertrend[curr] == True:\n",
    "            final_upperband[curr] = np.nan\n",
    "        else:\n",
    "            final_lowerband[curr] = np.nan\n",
    "    \n",
    "    #return pd.DataFrame({\n",
    "    #    'Supertrend': supertrend,\n",
    "    #    'Final Lowerband': final_lowerband,\n",
    "    #    'Final Upperband': final_upperband\n",
    "    #}, index=df.index)\n",
    "\n",
    "    df[\"SUPERTREND\"]=supertrend\n",
    "\n",
    "    return df\n",
    "\n",
    "#_____\n",
    "\n",
    "def suaviPlusDerivates(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global config\n",
    "\n",
    "    #_____UPDATE CONFIG PREDICTION\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "    short_ma_hyper=int(get_config(\"PARAMETERS\",\"SHORT_MA_HYPER\"))\n",
    "    short_ma_normal=int(get_config(\"PARAMETERS\",\"SHORT_MA_NORMAL\"))\n",
    "    long_ma_normal=int(get_config(\"PARAMETERS\",\"LONG_MA_NORMAL\"))\n",
    "    long_ma_hyper=int(get_config(\"PARAMETERS\",\"LONG_MA_HYPER\"))\n",
    "    ultra_long_ma_normal=int(get_config(\"PARAMETERS\",\"ULTRA_LONG_MA_NORMAL\"))\n",
    "\n",
    "    #_____VECTOR DE PRECIOS\n",
    "    close_data_df=candlesDataBase_BigQuery.copy()\n",
    "    close_data_df.CLOSE=close_data_df.CLOSE.astype(float)\n",
    "    close_data_df=close_data_df[[\"CLOSE\"]]\n",
    "\n",
    "    #_____SUAVIZACIONES MA & EMA\n",
    "    close_rolling_short_ma_hyper=close_data_df.rolling(short_ma_hyper).mean()\n",
    "    close_rolling_short_ma_normal=close_data_df.rolling(short_ma_normal).mean()\n",
    "    close_rolling_long_ma_normal=close_data_df.ewm(span=long_ma_normal).mean()\n",
    "    close_rolling_long_ma_hyper=close_data_df.ewm(span=long_ma_hyper).mean()\n",
    "    close_rolling_ultra_long_ma_normal=close_data_df.ewm(span=ultra_long_ma_normal).mean()\n",
    "\n",
    "    #_____INSERTAR NUEVAS VARIABLES\n",
    "    candlesDataBase_BigQuery[\"ROLLING_SHORT_MA_HYPER\"]=close_rolling_short_ma_hyper\n",
    "    candlesDataBase_BigQuery[\"ROLLING_SHORT_MA_NORMAL\"]=close_rolling_short_ma_normal\n",
    "    candlesDataBase_BigQuery[\"ROLLING_LONG_MA_NORMAL\"]=close_rolling_long_ma_normal\n",
    "    candlesDataBase_BigQuery[\"ROLLING_LONG_MA_HYPER\"]=close_rolling_long_ma_hyper\n",
    "    candlesDataBase_BigQuery[\"ROLLING_ULTRA_LONG_MA_NORMAL\"]=close_rolling_ultra_long_ma_normal\n",
    "\n",
    "    #_____BOLLINGER\n",
    "    candlesDataBase_BigQuery['15MA'] = candlesDataBase_BigQuery.groupby('MARKET')['CLOSE'].transform(lambda x: x.rolling(window=15).mean())\n",
    "    candlesDataBase_BigQuery['SD'] = candlesDataBase_BigQuery.groupby('MARKET')['CLOSE'].transform(lambda x: x.rolling(window=15).std())\n",
    "    candlesDataBase_BigQuery['UPPERBAND'] = candlesDataBase_BigQuery['15MA'] + 2*candlesDataBase_BigQuery['SD']\n",
    "    candlesDataBase_BigQuery['LOWERBAND'] = candlesDataBase_BigQuery['15MA'] - 2*candlesDataBase_BigQuery['SD']\n",
    "\n",
    "    #_____RATE CHANGE\n",
    "    candlesDataBase_BigQuery['RC'] = candlesDataBase_BigQuery.groupby('MARKET')['CLOSE'].transform(lambda x: x.pct_change(periods = 15))\n",
    "\n",
    "    #_____ELIMINAR DATOS FALTANTES\n",
    "    candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    #####\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_CLOSE= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_CLOSE = UnivariateSpline(x_CLOSE,candlesDataBase_BigQuery[[\"CLOSE\"]],s=0,k=3)\n",
    "    Y_1D_CLOSE=y_spl_CLOSE.derivative(n=1)\n",
    "    Y_1D_CLOSE=pd.DataFrame(Y_1D_CLOSE(x_CLOSE))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_CLOSE\"]=Y_1D_CLOSE\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_ROLLING_SHORT_MA_HYPER= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_ROLLING_SHORT_MA_HYPER = UnivariateSpline(x_ROLLING_SHORT_MA_HYPER,candlesDataBase_BigQuery[[\"ROLLING_SHORT_MA_HYPER\"]],s=0,k=3)\n",
    "    Y_1D_ROLLING_SHORT_MA_HYPER=y_spl_ROLLING_SHORT_MA_HYPER.derivative(n=1)\n",
    "    Y_1D_ROLLING_SHORT_MA_HYPER=pd.DataFrame(Y_1D_ROLLING_SHORT_MA_HYPER(x_ROLLING_SHORT_MA_HYPER))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_ROLLING_SHORT_MA_HYPER\"]=Y_1D_ROLLING_SHORT_MA_HYPER\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_ROLLING_SHORT_MA_NORMAL= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_ROLLING_SHORT_MA_NORMAL = UnivariateSpline(x_ROLLING_SHORT_MA_NORMAL,candlesDataBase_BigQuery[[\"ROLLING_SHORT_MA_NORMAL\"]],s=0,k=3)\n",
    "    Y_1D_ROLLING_SHORT_MA_NORMAL=y_spl_ROLLING_SHORT_MA_NORMAL.derivative(n=1)\n",
    "    Y_1D_ROLLING_SHORT_MA_NORMAL=pd.DataFrame(Y_1D_ROLLING_SHORT_MA_NORMAL(x_ROLLING_SHORT_MA_NORMAL))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_ROLLING_SHORT_MA_NORMAL\"]=Y_1D_ROLLING_SHORT_MA_NORMAL\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_ROLLING_LONG_MA_NORMAL= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_ROLLING_LONG_MA_NORMAL = UnivariateSpline(x_ROLLING_LONG_MA_NORMAL,candlesDataBase_BigQuery[[\"ROLLING_LONG_MA_NORMAL\"]],s=0,k=3)\n",
    "    Y_1D_ROLLING_LONG_MA_NORMAL=y_spl_ROLLING_LONG_MA_NORMAL.derivative(n=1)\n",
    "    Y_1D_ROLLING_LONG_MA_NORMAL=pd.DataFrame(Y_1D_ROLLING_LONG_MA_NORMAL(x_ROLLING_LONG_MA_NORMAL))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_ROLLING_LONG_MA_NORMAL\"]=Y_1D_ROLLING_LONG_MA_NORMAL\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_ROLLING_LONG_MA_HYPER= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_ROLLING_LONG_MA_HYPER = UnivariateSpline(x_ROLLING_LONG_MA_HYPER,candlesDataBase_BigQuery[[\"ROLLING_LONG_MA_HYPER\"]],s=0,k=3)\n",
    "    Y_1D_ROLLING_LONG_MA_HYPER=y_spl_ROLLING_LONG_MA_HYPER.derivative(n=1)\n",
    "    Y_1D_ROLLING_LONG_MA_HYPER=pd.DataFrame(Y_1D_ROLLING_LONG_MA_HYPER(x_ROLLING_LONG_MA_HYPER))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_ROLLING_LONG_MA_HYPER\"]=Y_1D_ROLLING_LONG_MA_HYPER\n",
    "\n",
    "    #_____CÁLCULO DE LA PRIMERA DERIVADA\n",
    "    x_ROLLING_ULTRA_LONG_MA_NORMAL= np.arange(len(candlesDataBase_BigQuery))\n",
    "    y_spl_ROLLING_ULTRA_LONG_MA_NORMAL = UnivariateSpline(x_ROLLING_ULTRA_LONG_MA_NORMAL,candlesDataBase_BigQuery[[\"ROLLING_ULTRA_LONG_MA_NORMAL\"]],s=0,k=3)\n",
    "    Y_1D_ROLLING_ULTRA_LONG_MA_NORMAL=y_spl_ROLLING_ULTRA_LONG_MA_NORMAL.derivative(n=1)\n",
    "    Y_1D_ROLLING_ULTRA_LONG_MA_NORMAL=pd.DataFrame(Y_1D_ROLLING_ULTRA_LONG_MA_NORMAL(x_ROLLING_ULTRA_LONG_MA_NORMAL))\n",
    "    candlesDataBase_BigQuery[\"Y_1D_ROLLING_ULTRA_LONG_MA_NORMAL\"]=Y_1D_ROLLING_ULTRA_LONG_MA_NORMAL\n",
    "\n",
    "    #####\n",
    "\n",
    "    #_____CALCULAR VARIABLE RSI\n",
    "    RSI_ROLLING_SHORT_MA_HYPER = RSI_GENERAL(pd.Series(list(candlesDataBase_BigQuery.ROLLING_SHORT_MA_HYPER.values)), int(get_config(\"PARAMETERS\",\"RSI_LEN\")))\n",
    "    RSI_ROLLING_SHORT_MA_NORMAL = RSI_GENERAL(pd.Series(list(candlesDataBase_BigQuery.ROLLING_SHORT_MA_NORMAL.values)), int(get_config(\"PARAMETERS\",\"RSI_LEN\")))\n",
    "    RSI_ROLLING_LONG_MA_NORMAL = RSI_GENERAL(pd.Series(list(candlesDataBase_BigQuery.ROLLING_LONG_MA_NORMAL.values)), int(get_config(\"PARAMETERS\",\"RSI_LEN\")))\n",
    "    RSI_ROLLING_LONG_MA_HYPER = RSI_GENERAL(pd.Series(list(candlesDataBase_BigQuery.ROLLING_LONG_MA_HYPER.values)), int(get_config(\"PARAMETERS\",\"RSI_LEN\")))\n",
    "    RSI_ROLLING_ULTRA_LONG_MA_NORMAL = RSI_GENERAL(pd.Series(list(candlesDataBase_BigQuery.ROLLING_ULTRA_LONG_MA_NORMAL.values)), int(get_config(\"PARAMETERS\",\"RSI_LEN\")))\n",
    "\n",
    "    #_____AGRAGAR VARIABLE RSI A BASE DE DATOS\n",
    "    candlesDataBase_BigQuery[\"RSI_ROLLING_SHORT_MA_HYPER\"]=RSI_ROLLING_SHORT_MA_HYPER\n",
    "    candlesDataBase_BigQuery[\"RSI_ROLLING_SHORT_MA_NORMAL\"]=RSI_ROLLING_SHORT_MA_NORMAL\n",
    "    candlesDataBase_BigQuery[\"RSI_ROLLING_LONG_MA_NORMAL\"]=RSI_ROLLING_LONG_MA_NORMAL\n",
    "    candlesDataBase_BigQuery[\"RSI_ROLLING_LONG_MA_HYPER\"]=RSI_ROLLING_LONG_MA_HYPER\n",
    "    candlesDataBase_BigQuery[\"RSI_ROLLING_ULTRA_LONG_MA_NORMAL\"]=RSI_ROLLING_ULTRA_LONG_MA_NORMAL\n",
    "\n",
    "    #_____REESTRUCTURAR BASE DE DATOS\n",
    "    candlesDataBase_BigQuery=candlesDataBase_BigQuery[int(get_config(\"PARAMETERS\",\"ULTRA_LONG_MA_NORMAL\"))+1:]\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE CALCULA MACD\n",
    "def LAGS(candlesDataBase_BigQuery):\n",
    "    \n",
    "    for j in list(candlesDataBase_BigQuery.columns)[3:]:\n",
    "        candlesDataBase_BigQuery[j+\"_1\"]=candlesDataBase_BigQuery[j].shift(1)\n",
    "        candlesDataBase_BigQuery[j+\"_2\"]=candlesDataBase_BigQuery[j].shift(2)\n",
    "        candlesDataBase_BigQuery[j+\"_3\"]=candlesDataBase_BigQuery[j].shift(3)\n",
    "        \n",
    "    candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#CREATE VARIABLES\n",
    "def variableCreation(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global fractal\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    candlesDataBase_BigQuery=RSI(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery=MACD(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery=fibonacciLevels(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery=newVariables(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery=superTrend(candlesDataBase_BigQuery, int(get_config(\"PARAMETERS\",\"ATR_PERIOD\")), int(get_config(\"PARAMETERS\",\"ATR_MULTIP\")))\n",
    "    candlesDataBase_BigQuery=suaviPlusDerivates(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery=LAGS(candlesDataBase_BigQuery)\n",
    "    \n",
    "    candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUINCIÓN QUE DEVUELVE EL ARREGLO DE LA RESPUESTA DEL MODELO\n",
    "def variableBuy(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global config\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    periods_ahead = int(get_config(\"PARAMETERS\",\"PERIODS_AHEAD\"))\n",
    "    \n",
    "    #_____DETERMINAR SI ESTOY EN BUENA ZONA DE COMPRA\n",
    "    #for i in range(0,len(candlesDataBase_BigQuery.CLOSE)-int(get_config(\"PARAMETERS\",\"PERIODS_TO_UTILITY\"))):\n",
    "    #    if (candlesDataBase_BigQuery.CLOSE.values[i]<=np.quantile(list(candlesDataBase_BigQuery.CLOSE.values[i:i+periods_ahead]),0.05)):\n",
    "    #        contador_utilidad_encontrada=0\n",
    "    #        for j in range(i+1,i+int(get_config(\"PARAMETERS\",\"PERIODS_TO_UTILITY\"))):\n",
    "    #            if candlesDataBase_BigQuery.CLOSE[j]/candlesDataBase_BigQuery.CLOSE[i]-1 >= float(get_config(\"PARAMETERS\",\"MIN_UTILITY\")):\n",
    "    #                contador_utilidad_encontrada=1\n",
    "    #        if contador_utilidad_encontrada==1:\n",
    "    #            candlesDataBase_BigQuery.at[i,\"BUY\"]=\"SI\"\n",
    "    #        else:\n",
    "    #            candlesDataBase_BigQuery.at[i,\"BUY\"]=\"NO\"\n",
    "    #    else:\n",
    "    #        candlesDataBase_BigQuery.at[i,\"BUY\"]=\"NO\"\n",
    "    \n",
    "    #_____DETERMINAR SI ESTOY EN BUENA ZONA DE COMPRA\n",
    "    for i in range(0,len(candlesDataBase_BigQuery.CLOSE)-int(get_config(\"PARAMETERS\",\"PERIODS_TO_UTILITY\"))):\n",
    "        contador_utilidad_encontrada=0\n",
    "        for j in range(i+1,i+int(get_config(\"PARAMETERS\",\"PERIODS_TO_UTILITY\"))):\n",
    "            if candlesDataBase_BigQuery.CLOSE[j]/candlesDataBase_BigQuery.CLOSE[i]-1 >= float(get_config(\"PARAMETERS\",\"MIN_UTILITY\")):\n",
    "                contador_utilidad_encontrada=1\n",
    "        if contador_utilidad_encontrada==1:\n",
    "            candlesDataBase_BigQuery.at[i,\"BUY\"]=\"SI\"\n",
    "        else:\n",
    "            candlesDataBase_BigQuery.at[i,\"BUY\"]=\"NO\"\n",
    "            \n",
    "        #___\n",
    "        \n",
    "        if i > 0:\n",
    "            if candlesDataBase_BigQuery.at[i,\"BUY\"]==\"SI\":\n",
    "                if candlesDataBase_BigQuery.CLOSE[i+1]<candlesDataBase_BigQuery.CLOSE[i]:\n",
    "                    candlesDataBase_BigQuery.at[i,\"BUY\"]=\"NO\"\n",
    "    #___\n",
    "    \n",
    "    candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "    candlesDataBase_BigQuery=candlesDataBase_BigQuery[periods_ahead:-int(get_config(\"PARAMETERS\",\"PERIODS_TO_UTILITY\"))]\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    #___\n",
    "    \n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE ESCALA LAS VARIABLES CONTINUAS DE LA BASE DE DATOS\n",
    "def scaleVariables(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global marketDict\n",
    "    \n",
    "    #_____ESCALAR VARIABLES\n",
    "    continuous_variables=list(candlesDataBase_BigQuery.columns)[3:-1]\n",
    "    marketVariables=list(candlesDataBase_BigQuery.columns)[0:3]\n",
    "    responseVariable=list(candlesDataBase_BigQuery.columns)[-1:]\n",
    "\n",
    "    #_____LISTA APPEND DE VARIABLES\n",
    "    listaAppend=[]\n",
    "\n",
    "    for i in continuous_variables:\n",
    "\n",
    "        try:\n",
    "\n",
    "            scaler=preprocessing.StandardScaler(with_mean=True).fit(np.array(candlesDataBase_BigQuery[[i]]))\n",
    "            scaled_data=pd.DataFrame(scaler.transform(candlesDataBase_BigQuery[[i]]))\n",
    "            scaled_data.columns=[i]\n",
    "            \n",
    "            marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i]={}\n",
    "            marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"STD\"]=np.std(list(candlesDataBase_BigQuery[[i]].values))\n",
    "            marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"MEAN\"]=np.mean(list(candlesDataBase_BigQuery[[i]].values))\n",
    "            \n",
    "            candlesDataBase_BigQuery[str(i)] = scaled_data[str(i)]\n",
    "            listaAppend.append(i)\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "\n",
    "    writePickleVariable(marketDict,\"marketDict\")\n",
    "    candlesDataBase_BigQuery=candlesDataBase_BigQuery[marketVariables+listaAppend+responseVariable]\n",
    "\n",
    "    #_____RETURN\n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DEVUELVE LAS COLUMNAS DESPUÉS DE HACER BORUTA, ROLLING, SCALING, BINARY SELECTION, LAGS\n",
    "def boruta(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global CLOSE\n",
    "    global alpha_boruta\n",
    "    \n",
    "    #_____COLUMNAS VARIABLE X\n",
    "    continuous_variables=list(candlesDataBase_BigQuery.columns)[3:-1]\n",
    "    \n",
    "    #_____UPDATE CONFIG PREDICTION\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    if int(get_config(\"PARAMETERS\",\"BORUTA\"))==1:\n",
    "\n",
    "        #_____VECTOR INPUT\n",
    "        X=candlesDataBase_BigQuery.copy()\n",
    "        X=X[continuous_variables]\n",
    "        my_columns=X.columns\n",
    "        X=X.values\n",
    "\n",
    "        #_____VECTOR OUTPUT\n",
    "        y = candlesDataBase_BigQuery.copy()\n",
    "        y=y[\"BUY\"].values.ravel()\n",
    "\n",
    "        #_____RANDOM FOREST BASE PARA LA SELECCIÓN\n",
    "        rf = RandomForestClassifier(min_samples_split=2,max_features=\"auto\",bootstrap=True,n_jobs=-1,n_estimators=200,max_depth=2,class_weight=\"balanced\")    \n",
    "\n",
    "        #_____SELECCIONADOR BORUTA\n",
    "        alpha_boruta=0.05\n",
    "\n",
    "        boruta_selector = BorutaPy(rf,max_iter=5000,n_estimators=200,alpha=alpha_boruta)\n",
    "        boruta_selector.fit(X,y)\n",
    "        features_selected=boruta_selector.n_features_\n",
    "        selected_columns=list(my_columns[boruta_selector.support_].values)\n",
    "\n",
    "        if len(selected_columns)==0:\n",
    "            selected_columns=continuous_variables\n",
    "            \n",
    "        writePickleVariable(selected_columns,\"variableList\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        #selected_columns=continuous_variables\n",
    "        #selected_columns=ast.literal_eval(get_config(\"PARAMETERS\",\"BORUTA_COLUMNS\"))\n",
    "        selected_columns=readPickleVariable(\"variableList\")\n",
    "    \n",
    "    CLOSE=candlesDataBase_BigQuery.copy()\n",
    "    CLOSE=CLOSE[[\"CLOSE\"]]\n",
    "    \n",
    "    return candlesDataBase_BigQuery[list(candlesDataBase_BigQuery.columns)[0:3]+selected_columns+list(candlesDataBase_BigQuery.columns)[-1:]]\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DEVUELVE EL MODELO DE PREDICCIÓN Y LAS BASES DE DATOS GLOBALES TEST Y TRAIN\n",
    "def train(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global rf_random\n",
    "    global train_model\n",
    "    global test_sample\n",
    "    global train_sample\n",
    "    \n",
    "    #_____UPDATE CONFIG PREDICTION\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "    #_____COLUMNAS VARIABLE X\n",
    "    continuous_variables=list(candlesDataBase_BigQuery.columns)[3:-1]\n",
    "    \n",
    "    #_____SEPARAR MUESTRA TRAIN DE TEST, SI SE TRATA DE UNA PRUEBA\n",
    "    train_sample=candlesDataBase_BigQuery.copy()\n",
    "    train_sample=train_sample.sample(frac=float(get_config(\"PARAMETERS\",\"TRAIN_PROPORTION\")),random_state=42)\n",
    "    \n",
    "    test_sample=candlesDataBase_BigQuery.copy()\n",
    "    test_sample=test_sample.loc[~test_sample.index.isin(list(train_sample.index.values))]\n",
    "    \n",
    "    train_sample.reset_index(inplace=True,drop=True)\n",
    "    test_sample.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    if int(get_config(\"PARAMETERS\",\"TRAIN\"))==1:\n",
    "\n",
    "        #_____VECTOR INPUT TRAIN\n",
    "        X_train=train_sample.copy()\n",
    "        X_train=X_train[continuous_variables]\n",
    "        X_train=X_train.values\n",
    "\n",
    "        #_____VECTOR OUTPUT\n",
    "        y_train = train_sample.copy()\n",
    "        y_train=y_train[\"BUY\"].values.ravel()\n",
    "\n",
    "        #_____RANDOM GRID SEARCH\n",
    "        # Number of trees in random forest\n",
    "        #n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 3)]\n",
    "        n_estimators = [2000]\n",
    "        # Number of features to consider at every split\n",
    "        #max_features = ['auto', 'sqrt']\n",
    "        max_features = ['auto']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(2, 10, num = 9)]\n",
    "        #max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        #min_samples_split = [2, 5, 10]\n",
    "        min_samples_split = [2]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        #min_samples_leaf = [1, 2, 4]\n",
    "        min_samples_leaf = [1]\n",
    "        # Method of selecting samples for training each tree\n",
    "        #bootstrap = [True, False]\n",
    "        bootstrap = [True]\n",
    "        # Create the random grid\n",
    "        param_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'class_weight': [\"balanced\"],\n",
    "                       'bootstrap': bootstrap}\n",
    "\n",
    "        #_____RANDOM FOREST DE ENTRENAMIENTO\n",
    "        rf=RandomForestClassifier(random_state = 42)\n",
    "        #rf_random = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "        rf_random = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, n_jobs = -1)\n",
    "        # Fit the random search model\n",
    "        rf_random.fit(X_train, y_train)\n",
    "\n",
    "        #_____ENTRENAR MODELOS CON MEJOR PARAMETRIZACIÓN\n",
    "        train_model = BaggingClassifier(base_estimator=RandomForestClassifier(min_samples_split=rf_random.best_params_[\"min_samples_split\"],\n",
    "                                                                              max_features=rf_random.best_params_[\"max_features\"],\n",
    "                                                                              bootstrap=rf_random.best_params_[\"bootstrap\"],\n",
    "                                                                              n_jobs=-1,\n",
    "                                                                              n_estimators=rf_random.best_params_[\"n_estimators\"],\n",
    "                                                                              max_depth=rf_random.best_params_[\"max_depth\"],\n",
    "                                                                              class_weight=rf_random.best_params_[\"class_weight\"]),random_state=42).fit(X_train, y_train)\n",
    "\n",
    "        writePickleVariable(train_model,\"train_model\")\n",
    "    \n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE ESCALA LAS VARIABLES DEL TEST CON LOS PARÁMETROS DEL TRAIN (MEDIA + SDT)\n",
    "def scaleTestVariables(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global marketDict\n",
    "    \n",
    "    marketDict=readPickleVariable(\"marketDict\")\n",
    "    \n",
    "    #_____ESCALAR VARIABLES\n",
    "    continuous_variables=list(candlesDataBase_BigQuery.columns)[3:-1]\n",
    "    marketVariables=list(candlesDataBase_BigQuery.columns)[0:3]\n",
    "    responseVariable=list(candlesDataBase_BigQuery.columns)[-1:]\n",
    "\n",
    "    #_____LISTA APPEND DE VARIABLES\n",
    "    listaAppend=[]\n",
    "\n",
    "    for i in continuous_variables:\n",
    "\n",
    "        try:\n",
    "\n",
    "            STD=marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"STD\"]\n",
    "            MEAN=marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"MEAN\"]\n",
    "            \n",
    "            candlesDataBase_BigQuery[str(i)] = candlesDataBase_BigQuery[str(i)]-MEAN\n",
    "            candlesDataBase_BigQuery[str(i)] = candlesDataBase_BigQuery[str(i)]/STD\n",
    "            listaAppend.append(i)\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "\n",
    "    candlesDataBase_BigQuery=candlesDataBase_BigQuery[marketVariables+listaAppend+responseVariable]\n",
    "\n",
    "    #_____RETURN\n",
    "    return candlesDataBase_BigQuery\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE ESCALA LAS VARIABLES DEL TEST CON LOS PARÁMETROS DEL TRAIN (MEDIA + SDT)\n",
    "def scaleTestVariables_NOBUY(candlesDataBase_BigQuery):\n",
    "    \n",
    "    global marketDict\n",
    "    \n",
    "    marketDict=readPickleVariable(\"marketDict\")\n",
    "    \n",
    "    #_____ESCALAR VARIABLES\n",
    "    continuous_variables=list(candlesDataBase_BigQuery.columns)[3:-1]\n",
    "    marketVariables=list(candlesDataBase_BigQuery.columns)[0:3]\n",
    "\n",
    "    #_____LISTA APPEND DE VARIABLES\n",
    "    listaAppend=[]\n",
    "\n",
    "    for i in continuous_variables:\n",
    "\n",
    "        try:\n",
    "\n",
    "            STD=marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"STD\"]\n",
    "            MEAN=marketDict[candlesDataBase_BigQuery.at[0,\"MARKET\"]][i][\"MEAN\"]\n",
    "            \n",
    "            candlesDataBase_BigQuery[str(i)] = candlesDataBase_BigQuery[str(i)]-MEAN\n",
    "            candlesDataBase_BigQuery[str(i)] = candlesDataBase_BigQuery[str(i)]/STD\n",
    "            listaAppend.append(i)\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "\n",
    "    candlesDataBase_BigQuery=candlesDataBase_BigQuery[marketVariables+listaAppend]\n",
    "\n",
    "    #_____RETURN\n",
    "    return candlesDataBase_BigQuery\n",
    "    \n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE PREDICE SOBRE LOS DATOS TEST\n",
    "def predictFunction():\n",
    "\n",
    "    global train_model\n",
    "    global test_sample\n",
    "    \n",
    "    #_____TRAIN_MODEL\n",
    "    train_model=readPickleVariable(\"train_model\")\n",
    "    \n",
    "    #_____COLUMNAS VARIABLE X\n",
    "    continuous_variables=list(test_sample.columns)[3:-1]\n",
    "    \n",
    "    #_____DATOS\n",
    "    X_test=test_sample.copy()\n",
    "    X_test=X_test[continuous_variables]\n",
    "    X_test=X_test.values\n",
    "    \n",
    "    #_____PREDICTION\n",
    "    prediction = train_model.predict(np.array(X_test))\n",
    "    \n",
    "    #_____RETURN\n",
    "    return prediction\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE CORRIGE PREDICCIÓN DADO QUE EL PRECIO EN EL QUE ME ENCUENTRO ES MENOR AL PRECIO ANTERIOR\n",
    "def correctPredict(test_sample,PERIODS_EVALUATION_PAST,MAX_PERIODS_EVALUATION_QUANTILE,PERIODS_LINEAREG_BEFORE):\n",
    "    \n",
    "    global myRSI\n",
    "    \n",
    "    #_____ACTUALIZAR CONFIG\n",
    "    updateConfig(\"/home/ubuntu/SurfNet/predictionModels/PREDICTION/predictionModels.ini\")\n",
    "    \n",
    "    #_____VARAIABLE QUE PARAMETRIZA PSADO DE EVALUACIÓN\n",
    "    past_evaluation=PERIODS_EVALUATION_PAST\n",
    "    \n",
    "    #_____QUANLIT DE AVIALUACIÓN DE PRECIO MÁXIMO\n",
    "    max_past_evaluation_quantile=MAX_PERIODS_EVALUATION_QUANTILE\n",
    "    \n",
    "    #_____SI RSI ES MAYOR A THRESHOLD, NO COMPRAR\n",
    "    for i in range(0,len(test_sample)):\n",
    "        if (test_sample.at[i,\"PRED\"]==\"SI\") and (myRSI[i]>float(get_config(\"PARAMETERS\",\"MY_RSI\"))):\n",
    "            test_sample.at[i,\"PRED\"]=\"NO\"\n",
    "    \n",
    "    #_____RECORRER DATOS DE TEST\n",
    "    for i in range(past_evaluation,len(test_sample)):\n",
    "        \n",
    "        #_____SI LA PREDICCIÓN FUE COMPRAR & SI CLOSE ANTERIOR ES MAYOR\n",
    "        if (test_sample.at[i,\"PRED\"]==\"SI\") and (test_sample.at[i,\"CLOSE\"]>=np.quantile(test_sample[i-past_evaluation:i].CLOSE.values,max_past_evaluation_quantile)):\n",
    "            \n",
    "            #_____CONVERTIR OBSERVACIÓN EN NO COMRPAR\n",
    "            test_sample.at[i,\"PRED\"]=\"NO\"\n",
    "            \n",
    "    #_____RECORRER DATOS DE TEST\n",
    "    periods_lineareg=PERIODS_LINEAREG_BEFORE\n",
    "    for i in range(periods_lineareg,len(test_sample)):\n",
    "            \n",
    "        if (test_sample.at[i,\"PRED\"]==\"SI\"):\n",
    "\n",
    "            periods_lineareg=PERIODS_LINEAREG_BEFORE\n",
    "\n",
    "            X=np.arange(periods_lineareg).reshape(-1, 1)\n",
    "            Y=test_sample[i-periods_lineareg:i].CLOSE.values.reshape(-1, 1)\n",
    "\n",
    "            linear_regressor = LinearRegression()\n",
    "            nuevaVariable=linear_regressor.fit(X, Y)\n",
    "            Y_pred = linear_regressor.predict(X)\n",
    "\n",
    "            #_____CORREGIR\n",
    "            if nuevaVariable.coef_ < 0:\n",
    "\n",
    "                #_____CONVERTIR OBSERVACIÓN EN NO COMRPAR\n",
    "                test_sample.at[i,\"PRED\"]=\"NO\"\n",
    "                \n",
    "    #_____REESTRUCTURAR TEST SAMPLE\n",
    "    test_sample=test_sample[max(past_evaluation,periods_lineareg)+1:]\n",
    "    test_sample.reset_index(inplace=True,drop=True)\n",
    "                \n",
    "    return test_sample\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE HACE BACKTESTING PARA DETERMINAR MEJORES PARÁMETROS TOPES DE VENTA\n",
    "def backtesting(test_sample):\n",
    "    \n",
    "    global fractal\n",
    "    global final_df\n",
    "    global utility_df\n",
    "    global train_model\n",
    "    global tradingBook\n",
    "    global market_model\n",
    "    global train_sample\n",
    "    global backtestingDataFrame\n",
    "    \n",
    "    global output_tradingBook\n",
    "    global output_utility\n",
    "    \n",
    "    #_____UPDATE CONFIG PREDICTION\n",
    "    updateConfig(\"predictionModels.ini\")\n",
    "    \n",
    "    #_____INICIAR LISTAS\n",
    "    TP_LIST=ast.literal_eval(get_config(\"PARAMETERS\",\"TP_LIST\"))\n",
    "    SL_LIST=ast.literal_eval(get_config(\"PARAMETERS\",\"SL_LIST\"))\n",
    "    \n",
    "    #_____INICIAR PARÁMETROS\n",
    "    PERIODS_EVALUATION_PAST=ast.literal_eval(get_config(\"PARAMETERS\",\"PERIODS_EVALUATION_PAST\"))\n",
    "    MAX_PERIODS_EVALUATION_QUANTILE=ast.literal_eval(get_config(\"PARAMETERS\",\"MAX_PERIODS_EVALUATION_QUANTILE\"))\n",
    "    PERIODS_LINEAREG_BEFORE=ast.literal_eval(get_config(\"PARAMETERS\",\"PERIODS_LINEAREG_BEFORE\"))\n",
    "    \n",
    "    #_____INICIALIZAR BASE DE DATOS DE SIMULACIÓN DE UTILIDAD\n",
    "    utility_df=pd.DataFrame()\n",
    "\n",
    "    #_____LOOP DE SIMULACIÓN\n",
    "    for past in PERIODS_EVALUATION_PAST:\n",
    "        for quantile in MAX_PERIODS_EVALUATION_QUANTILE:\n",
    "            for lineareg in PERIODS_LINEAREG_BEFORE:\n",
    "                for takeprofit in TP_LIST:\n",
    "                    for stoploss in SL_LIST:\n",
    "                        \n",
    "                        #_____COPIAR BASE DE DATOS\n",
    "                        test_sample_copy=test_sample.copy()\n",
    "\n",
    "                        #_____SOBREESCRIBIR VARIABLE CLOSE CON VALORES REALES\n",
    "                        for back in range(0,len(test_sample_copy)):\n",
    "                            test_sample_copy.at[back,\"CLOSE\"]=backtestingDataFrame.loc[backtestingDataFrame.TIME==test_sample_copy.at[back,\"TIME\"]][\"CLOSE\"].values[0]\n",
    "                        \n",
    "                        #_____CORREGIR PREDICTION\n",
    "                        test_sample_copy=correctPredict(test_sample_copy,past,quantile,lineareg)\n",
    "                        predict=test_sample_copy[\"PRED\"]\n",
    "                        \n",
    "                        #_____INICIAR PARÁMETROS\n",
    "                        date=test_sample_copy[\"TIME\"]\n",
    "                        close_price=test_sample_copy[\"CLOSE\"]\n",
    "                        \n",
    "                        #_____CREAR LIBRO DE TRADEO\n",
    "                        tradingBook=pd.DataFrame(columns=[\"DATE_BUY\",\"DATE_SELL\",\"NET_BUY_PRICE\",\"NET_SELL_PRICE\",\"STOP_LOSS\",\"NET_UTILITY\"])\n",
    "\n",
    "                        #_____PARÁMETROS DE INICIO\n",
    "                        j=0\n",
    "                        ejecuto_compra=0\n",
    "\n",
    "                        #_____LOOP SEMI-INFINITO\n",
    "                        while j<=len(close_price)-2 :\n",
    "\n",
    "                            #_____TRIGGER DE COMPRA\n",
    "                            if predict[j] == \"SI\" :\n",
    "                                buy_authorization = \"SI\"\n",
    "                            else:\n",
    "                                buy_authorization = \"NO\"\n",
    "\n",
    "                            #_____SI TRIGGER DE COMPRA SE DISPARA\n",
    "                            if buy_authorization == \"SI\":\n",
    "\n",
    "                                #_____AGREGAR DATOS A LIBRO DE TRADEO + SUMAR UN POCO MÁS DE FEES PARA AJUSTAR A VALORES REALES\n",
    "                                time_buy=date[j]\n",
    "                                index_row=len(tradingBook)\n",
    "                                tradingBook.at[index_row,\"DATE_BUY\"]=time_buy\n",
    "                                tradingBook.at[index_row,\"DATE_SELL\"]=\"\"\n",
    "                                tradingBook.at[index_row,\"NET_BUY_PRICE\"]=close_price[j]*(1+0.001)\n",
    "\n",
    "                                #_____LOOP INFINITO HASTA QUE SE EJECUTE VENTA\n",
    "                                ejecuto_venta=0 \n",
    "                                while ejecuto_venta==0 :\n",
    "\n",
    "                                    #_____SI PRECIO COMIENZA A CAER                \n",
    "                                    if (ejecuto_venta==0) and j<=(len(close_price)-2):\n",
    "\n",
    "                                        #_____ACTUALIZAR TIEMPO TRANSCURRIDO\n",
    "                                        j+=1\n",
    "\n",
    "                                        #_____ACTUALIZAR MARGEN DE VENTA + SUMAR UN POCO MÁS DE FEES PARA AJUSTAR A VALORES REALES\n",
    "                                        margen_actual= ((close_price[j]*(1-0.001))/tradingBook.at[index_row,\"NET_BUY_PRICE\"])-1\n",
    "\n",
    "                                        #_____VENDER SI ESTOY ARRIBA DE MARGEN DE UTLIDAD\n",
    "                                        if (ejecuto_venta==0) and (margen_actual >= takeprofit):\n",
    "\n",
    "                                            #_____EJECUTAR ORDEN DE VENTA\n",
    "                                            ejecuto_venta=1\n",
    "\n",
    "                                            #_____ACTUALIZAR BASE DE DATOS TRADINGBOOK_1\n",
    "                                            tradingBook.at[index_row,\"DATE_SELL\"]=date[j]\n",
    "                                            tradingBook.at[index_row,\"NET_SELL_PRICE\"]=(close_price[j]*(1-0.001))\n",
    "                                            tradingBook.at[index_row,\"STOP_LOSS\"]=0\n",
    "                                            tradingBook.at[index_row,\"NET_UTILITY\"]=(tradingBook.at[index_row,\"NET_SELL_PRICE\"]/tradingBook.at[index_row,\"NET_BUY_PRICE\"])-1\n",
    "\n",
    "                                        #_____SI PRECIO ESTÁ EN REGIÓN DE STOPLOSS\n",
    "                                        elif (ejecuto_venta==0) and ((close_price[j] < tradingBook.at[index_row,\"NET_BUY_PRICE\"]*(1-stoploss+0.001))):\n",
    "\n",
    "                                            #_____EJECUTAR ORDEN DE VENTA\n",
    "                                            ejecuto_venta=1\n",
    "\n",
    "                                            #_____ACTUALIZAR BASE DE DATOS TRADINGBOOK_1\n",
    "                                            tradingBook.at[index_row,\"DATE_SELL\"]=date[j]\n",
    "                                            tradingBook.at[index_row,\"NET_SELL_PRICE\"]=close_price[j]\n",
    "                                            tradingBook.at[index_row,\"STOP_LOSS\"]=1\n",
    "                                            tradingBook.at[index_row,\"NET_UTILITY\"]=((close_price[j]*(1-0.001))/tradingBook.at[index_row,\"NET_BUY_PRICE\"])-1\n",
    "\n",
    "                                    else:\n",
    "                                        ejecuto_venta=1\n",
    "\n",
    "                            else:\n",
    "                                #_____ACTUALIZAR TIEMPO TRANSCURRIDO\n",
    "                                j+=1\n",
    "\n",
    "                        #_____AGREGAR DATOS A BASE DE DATOS DE UTILIDAD\n",
    "                        index_row=len(utility_df)\n",
    "                        utility_df.at[index_row,\"TP\"]=takeprofit\n",
    "                        utility_df.at[index_row,\"SL\"]=stoploss\n",
    "                        utility_df.at[index_row,\"PERIODS_EVALUATION_PAST\"]=past\n",
    "                        utility_df.at[index_row,\"MAX_PERIODS_EVALUATION_QUANTILE\"]=quantile\n",
    "                        utility_df.at[index_row,\"PERIODS_LINEAREG_BEFORE\"]=lineareg\n",
    "                        utility_df.at[index_row,\"NET_UTILITY\"]=sum(tradingBook.dropna().NET_UTILITY)\n",
    "                        \n",
    "    #_____OUTPUT\n",
    "    output_tradingBook=tradingBook.copy()\n",
    "    output_utility=utility_df.copy()\n",
    "    \n",
    "    #_____FILTRAR BASE DE DATOS POR UTILIDAD MÁS ALTA CONSEGUIDA\n",
    "    final_df=utility_df.loc[utility_df.NET_UTILITY==max(list(utility_df.NET_UTILITY.values))]\n",
    "    final_df.reset_index(inplace=True,drop=True)\n",
    "    final_df=final_df[0:1]\n",
    "    final_df.at[0,\"AVG_NET_UTILITY\"]=np.mean(list(utility_df.NET_UTILITY.values))\n",
    "    final_df.at[0,\"MAX_NET_UTILITY_DAY\"]=max(list(utility_df.NET_UTILITY.values))/(len(test_sample)*int(fractal)/60/24)\n",
    "    final_df.at[0,\"TRAIN_COLUMNS\"]=str(readPickleVariable(\"variableList\"))\n",
    "    \n",
    "    #_____TRATAR DE DESCARGAR BASE DE DATOS \"EXISTENTE\"\n",
    "    try:\n",
    "        final_df_PAST=readPickleVariable(\"/home/ubuntu/SurfNet/predictionModels/MODELS/\"+market_model+\"_PARAMETERS\"+\"_\"+fractal)\n",
    "    except:\n",
    "        \n",
    "        #_____CREAR BASE DE DATOS INICIAL\n",
    "        final_df_PAST=pd.DataFrame()\n",
    "        final_df_PAST.at[0,\"TP\"]=0.0\n",
    "        final_df_PAST.at[0,\"SL\"]=0.0\n",
    "        final_df_PAST.at[0,\"PERIODS_EVALUATION_PAST\"]=0.0\n",
    "        final_df_PAST.at[0,\"MAX_PERIODS_EVALUATION_QUANTILE\"]=0.0\n",
    "        final_df_PAST.at[0,\"PERIODS_LINEAREG_BEFORE\"]=0.0\n",
    "        final_df_PAST.at[0,\"NET_UTILITY\"]=0.0\n",
    "        final_df_PAST.at[0,\"AVG_NET_UTILITY\"]=0.0\n",
    "        final_df_PAST.at[0,\"MAX_NET_UTILITY_DAY\"]=0.0\n",
    "        final_df_PAST.at[0,\"TRAIN_COLUMNS\"]=\"\"\n",
    "        writePickleVariable(final_df_PAST,\"/home/ubuntu/SurfNet/predictionModels/MODELS/\"+market_model+\"_PARAMETERS\"+\"_\"+fractal)\n",
    "\n",
    "    #_____SI MODELO ACTUAL ES MEJOR QUE MODELO ANTERIOR\n",
    "    if (final_df.at[0,\"NET_UTILITY\"]>0) and (final_df.at[0,\"AVG_NET_UTILITY\"]>0):\n",
    "        \n",
    "        if final_df.at[0,\"MAX_NET_UTILITY_DAY\"]>=float(get_config(\"PARAMETERS\",\"MINIMUM_UTILITY_DAY\")):\n",
    "        \n",
    "            #_____ESCRIBIR NUEVOS PARÁMETROS PICKLE\n",
    "            writePickleVariable(final_df,\"/home/ubuntu/SurfNet/predictionModels/MODELS/\"+market_model+\"_PARAMETERS\"+\"_\"+fractal)\n",
    "    \n",
    "#_____\n",
    "    \n",
    "#FINCIÓN QUE FILTRA MERCADOS POTENCIALES POR VOLUMEN TRANSADO\n",
    "def marketList():\n",
    "    \n",
    "    #_____CONEXIÓN BINANCE\n",
    "    binance = ccxt.binance()\n",
    "\n",
    "    #_____ACTUALIZAR ARCHIVO CONFIG PARA MERCADOS\n",
    "    updateConfig('/home/ubuntu/SurfNet/candleDataBases/candleDataBases.ini')\n",
    "\n",
    "    #_____CREAR LISTA DE MERCADOS A EVALUAR\n",
    "    mercados_Portafolio_Bruto=[]\n",
    "    mercados=ast.literal_eval(get_config(\"DATABASES\",\"MARKETS\"))\n",
    "\n",
    "    #_____LISTA VOLUMEN MERCADOS\n",
    "    marketZeroVolumes_Prop=[]\n",
    "    marketAverVolumes_List=[]\n",
    "\n",
    "    #_____LISTA SAFE DE MERCADOS\n",
    "    safeList=[]\n",
    "    \n",
    "    #_____LISTA TOP DE MERCADOS CMC\n",
    "    cmc_Top_Market_List=topCoinMarketCap()\n",
    "\n",
    "    #_____RECORRER MERCADOS\n",
    "    for mercado in mercados:\n",
    "\n",
    "        #_____DESCARGAR BASE DE DATOS OLCV\n",
    "        #candlesDataBase_Binance = binance.fetch_ohlcv(mercado+\"/USDT\",\"1m\",limit=1000)\n",
    "        #candlesDataBase_Binance = candlesDataBase_Binance[:-1]\n",
    "\n",
    "        #_____PROPORCIÓN MERCADO MUERTO\n",
    "        #contador_cero=0\n",
    "        #contador_aver=0\n",
    "        #for i in list(range(0,len(candlesDataBase_Binance))):\n",
    "        #    if candlesDataBase_Binance[i][5] == 0:\n",
    "        #        contador_cero=contador_cero+1\n",
    "        #    contador_aver=contador_aver+candlesDataBase_Binance[i][5]\n",
    "\n",
    "        #marketZeroVolumes_Prop.append(contador_cero/len(candlesDataBase_Binance))\n",
    "        #marketAverVolumes_List.append(contador_aver/len(candlesDataBase_Binance))\n",
    "\n",
    "        #_____ACTUALIZAR SAFE LIST\n",
    "        #if ((contador_cero/len(candlesDataBase_Binance)) == 0):\n",
    "        #    safeList.append(mercado)\n",
    "        \n",
    "        #_____SLEEP\n",
    "        #time.sleep(2)\n",
    "        \n",
    "        #_____AGEREGAR MERCADO SI ESTÁ EN TOP CMC\n",
    "        if mercado in cmc_Top_Market_List:\n",
    "            safeList.append(mercado)\n",
    "        \n",
    "\n",
    "    #_____NUEVA LISTA DE MERCADOS\n",
    "    return safeList\n",
    "\n",
    "#_____\n",
    "\n",
    "#FUNCIÓN QUE DEVUELVE EL TOG X DE MERCADOS\n",
    "def topCoinMarketCap():\n",
    "    \n",
    "    updateConfig('/home/ubuntu/SurfNet/surfNetRun/semiManual.ini')\n",
    "    \n",
    "    API_KEY_CAP=\"8edb78ea-b23d-4b23-8a6f-5ece6b703720\"\n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n",
    "    parameters = {\n",
    "      'start':'1',\n",
    "      'limit': get_config(\"MARKETS\",\"TOP_CMC\"),\n",
    "      'convert':'USD'\n",
    "    }\n",
    "    headers = {\n",
    "      'Accepts': 'application/json',\n",
    "      'X-CMC_PRO_API_KEY': API_KEY_CAP,\n",
    "    }\n",
    "\n",
    "    session = Session()\n",
    "    session.headers.update(headers)\n",
    "\n",
    "    try:\n",
    "      response = session.get(url, params=parameters)\n",
    "      data = json.loads(response.text)\n",
    "      #print(data)\n",
    "    except (ConnectionError, Timeout, TooManyRedirects) as e:\n",
    "      print(e)\n",
    "    \n",
    "    response = pd.DataFrame(data[\"data\"])\n",
    "    \n",
    "    return list(response.symbol.values)\n",
    "\n",
    "#_____\n",
    "\n",
    "#TWITTER CONNECTION\n",
    "def OAuth():\n",
    "    global auth\n",
    "    API_KEY=\"gjxBV6ksuiYXxAN8PZ6LmWMZm\"\n",
    "    API_SECRET_KEY=\"SahRqwCN26NwIF2ZnuWNqVKMHbW8zCBGdQMbfm6qGOu85exbUa\"\n",
    "    ACCESS_TOKEN=\"1391754949026004993-060MZPsduBwRYKjEeCYCeH5jLg1yZZ\"\n",
    "    ACCESS_TOKEN_SECRET=\"WZabC59NE88jYVvDxJxNuVp5Q6vu6jRCLn2E2YdrCEGBu\"\n",
    "    #API_KEY=\"Zb2EpaerwrtVlp7ekZ5QbXD8n\"\n",
    "    #API_SECRET_KEY=\"3R2jerQ542EAwlo4yjNjvxwGNzXKwUxxTIcvPyPmDH8VUos6ht\"\n",
    "    #ACCESS_TOKEN=\"1472968680564736010-WEVntWf9cQ8N4ID4MGJUvsRU1A1Pjj\"\n",
    "    #ACCESS_TOKEN_SECRET=\"sM8tNMLkNIZpuaTNqDRV4VlWsDWwHNpWvYuWAjuZmLpOv\"\n",
    "    \n",
    "    try:\n",
    "        auth = tw.OAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "        auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "        return auth\n",
    "\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____UPDATE CONFIG PREDICTION\n",
    "# updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "# fractal=\"15\"\n",
    "# market_model=\"LUNA_USDT_1M\"\n",
    "\n",
    "# #_____CARGAR BASE DE DATOS\n",
    "# candlesDataBase_BigQuery=downloadDataBaseBigQuery(market_model,time=fractal,rows=int(get_config(\"PARAMETERS\",\"ROWS_DOWNLOAD\")))\n",
    "\n",
    "# #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "# backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "# #_____CREAR VARIABLES INPUT\n",
    "# candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "# #_____CREAR VARIABLE DE RESPUESTA\n",
    "# candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "# candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "# candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# #_____ESCALAR VARIABLES\n",
    "# # candlesDataBase_BigQuery=scaleVariables(candlesDataBase_BigQuery)\n",
    "# candlesDataBase_BigQuery=scaleTestVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "# #_____FEATURE SELECTION\n",
    "# print(len(list(candlesDataBase_BigQuery.columns)))\n",
    "# candlesDataBase_BigQuery=boruta(candlesDataBase_BigQuery)\n",
    "# print(len(list(candlesDataBase_BigQuery.columns)))\n",
    "\n",
    "# #_____TRAIN MODEL\n",
    "# train(candlesDataBase_BigQuery)\n",
    "\n",
    "# #_____PREDECIR VALORES DE TESTEO\n",
    "# test_sample[\"PRED\"]=predictFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____PARÁMETROS\n",
    "# candlesDataBase_Graph=test_sample.copy()\n",
    "# predType=\"BUY\"\n",
    "\n",
    "# #_____AGREGAR VARIABLE CLOSE\n",
    "# for i in range(0,len(candlesDataBase_Graph)):\n",
    "#     candlesDataBase_Graph.at[i,\"CLOSE\"]=backtestingDataFrame.loc[backtestingDataFrame.TIME==candlesDataBase_Graph.at[i,\"TIME\"]][\"CLOSE\"].values[0]\n",
    "\n",
    "# #_____AGREGAR COLORES\n",
    "# candlesDataBase_Color=candlesDataBase_Graph.copy()\n",
    "# candlesDataBase_Color=candlesDataBase_Color[[predType]]\n",
    "# candlesDataBase_Color=np.where(candlesDataBase_Color[predType] ==\"SI\", \"red\", \"blue\")\n",
    "\n",
    "# #_____DAR TEXTO A EJES\n",
    "# x_buy = candlesDataBase_Graph[[\"TIME\"]].values\n",
    "# y_buy = candlesDataBase_Graph[[\"CLOSE\"]].values\n",
    "# color_buy = candlesDataBase_Color\n",
    "\n",
    "# #_____RESETEAR\n",
    "# fig_buy = None\n",
    "\n",
    "# #_____IMPRIMIR GRÁFICA\n",
    "# fig_buy = plt.figure(1, figsize=(20,10))\n",
    "# ax_buy  = fig_buy.add_subplot(111)\n",
    "# plot_colourline(x_buy,y_buy,color_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____PARÁMETROS\n",
    "# candlesDataBase_Graph=test_sample.copy()\n",
    "# predType=\"PRED\"\n",
    "\n",
    "# #_____AGREGAR VARIABLE CLOSE\n",
    "# for i in range(0,len(candlesDataBase_Graph)):\n",
    "#     candlesDataBase_Graph.at[i,\"CLOSE\"]=backtestingDataFrame.loc[backtestingDataFrame.TIME==candlesDataBase_Graph.at[i,\"TIME\"]][\"CLOSE\"].values[0]\n",
    "\n",
    "# #_____ZOOM\n",
    "# # candlesDataBase_Graph=candlesDataBase_Graph[len(candlesDataBase_Graph)-400:len(candlesDataBase_Graph)-300]\n",
    "# # candlesDataBase_Graph=candlesDataBase_Graph[len(candlesDataBase_Graph)-400:]\n",
    "# # candlesDataBase_Graph.reset_index(inplace=True)\n",
    "\n",
    "# #_____AGREGAR COLORES\n",
    "# candlesDataBase_Color=candlesDataBase_Graph.copy()\n",
    "# candlesDataBase_Color=candlesDataBase_Color[[predType]]\n",
    "# candlesDataBase_Color=np.where(candlesDataBase_Color[predType] ==\"SI\", \"red\", \"blue\")\n",
    "\n",
    "# #_____DAR TEXTO A EJES\n",
    "# x_buy = candlesDataBase_Graph[[\"TIME\"]].values\n",
    "# y_buy = candlesDataBase_Graph[[\"CLOSE\"]].values\n",
    "# color_buy = candlesDataBase_Color\n",
    "\n",
    "# #_____RESETEAR\n",
    "# fig_buy = None\n",
    "\n",
    "# #_____IMPRIMIR GRÁFICA\n",
    "# fig_buy = plt.figure(1, figsize=(20,10))\n",
    "# ax_buy  = fig_buy.add_subplot(111)\n",
    "# plot_colourline(x_buy,y_buy,color_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE MARKETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT MARKETS WITH VOLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_____CCXT\n",
    "binance = ccxt.binance()\n",
    "\n",
    "#_____GOOGLE CLOUD CONECTION\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/ubuntu/SurfNet/predictionModels/PREDICTION/bigQueryAccess.json\"\n",
    "\n",
    "# #_____ACTUALIZAR ARCHIVO CONFIG PARA MERCADOS\n",
    "# updateConfig('/home/ubuntu/SurfNet/candleDataBases/candleDataBases.ini')\n",
    "\n",
    "# #_____CREAR LISTA DE MERCADOS A EVALUAR\n",
    "# mercados_Portafolio_Bruto=[]\n",
    "# mercados=ast.literal_eval(get_config(\"DATABASES\",\"MARKETS\"))\n",
    "\n",
    "# #_____LISTA VOLUMEN MERCADOS\n",
    "# marketZeroVolumes_Prop=[]\n",
    "# marketAverVolumes_List=[]\n",
    "\n",
    "# #_____LISTA SAFE DE MERCADOS\n",
    "# safeList=[]\n",
    "\n",
    "# #_____RECORRER MERCADOS\n",
    "# for mercado in mercados:\n",
    "\n",
    "#     #_____DESCARGAR BASE DE DATOS OLCV\n",
    "#     candlesDataBase_Binance = binance.fetch_ohlcv(mercado+\"/USDT\",\"1m\",limit=1000)\n",
    "#     candlesDataBase_Binance = candlesDataBase_Binance[:-1]\n",
    "\n",
    "#     #_____PROPORCIÓN MERCADO MUERTO\n",
    "#     contador_cero=0\n",
    "#     contador_aver=0\n",
    "#     for i in list(range(0,len(candlesDataBase_Binance))):\n",
    "#         if candlesDataBase_Binance[i][5] == 0:\n",
    "#             contador_cero=contador_cero+1\n",
    "#         contador_aver=contador_aver+candlesDataBase_Binance[i][5]\n",
    "\n",
    "#     marketZeroVolumes_Prop.append(contador_cero/len(candlesDataBase_Binance))\n",
    "#     marketAverVolumes_List.append(contador_aver/len(candlesDataBase_Binance))\n",
    "\n",
    "#     #_____ACTUALIZAR SAFE LIST\n",
    "#     if ((contador_cero/len(candlesDataBase_Binance)) == 0):\n",
    "#         safeList.append(mercado)\n",
    "\n",
    "#     #_____SLEEP\n",
    "#     time.sleep(1)\n",
    "\n",
    "# #_____NUEVA LISTA DE MERCADOS\n",
    "# mercados=safeList\n",
    "\n",
    "# #_____DICCIONARIO DE MERCADOS\n",
    "# marketDict={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGE DATAFRAME OF DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____UPDATE CONFIG PREDICTION\n",
    "# updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "# #_____TOTAL DATAFRAME\n",
    "# candlesDataBase_BigQuery_Total=pd.DataFrame()\n",
    "\n",
    "# for i in mercados:\n",
    "    \n",
    "#     fractal=\"15\"\n",
    "#     market_model=i+\"_USDT_1M\"\n",
    "\n",
    "#     #_____CARGAR BASE DE DATOS\n",
    "#     candlesDataBase_BigQuery=downloadDataBaseBigQuery(market_model,time=fractal,rows=int(get_config(\"PARAMETERS\",\"ROWS_DOWNLOAD\")))\n",
    "\n",
    "#     #_____FILTRAR POR REGISTROS\n",
    "#     if len(candlesDataBase_BigQuery)>int(get_config(\"PARAMETERS\",\"MIN_ROWS\")):\n",
    "#         marketDict[i+\"/USDT\"]={}\n",
    "    \n",
    "#         #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "#         backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "#         #_____CREAR VARIABLES INPUT\n",
    "#         candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "#         #_____CREAR VARIABLE DE RESPUESTA\n",
    "#         candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "#         candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "#         candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#         #_____ESCALAR VARIABLES\n",
    "#         candlesDataBase_BigQuery=scaleVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "#         #_____APPEND TO BIG DATAFRAME\n",
    "#         candlesDataBase_BigQuery_Total=candlesDataBase_BigQuery_Total.append(candlesDataBase_BigQuery[:-1000])\n",
    "\n",
    "#         clear_output()\n",
    "#         print(len(candlesDataBase_BigQuery_Total))\n",
    "    \n",
    "# candlesDataBase_BigQuery_Total_BackUp=candlesDataBase_BigQuery_Total.copy()\n",
    "# candlesDataBase_BigQuery_Total_BackUp.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #_____BACKUP\n",
    "# candlesDataBase_BigQuery_Total=candlesDataBase_BigQuery_Total_BackUp.copy()\n",
    "\n",
    "# #_____FEATURE SELECTION\n",
    "# print(len(list(candlesDataBase_BigQuery_Total.columns)))\n",
    "# candlesDataBase_BigQuery_Total=boruta(candlesDataBase_BigQuery_Total)\n",
    "# print(len(list(candlesDataBase_BigQuery_Total.columns)))\n",
    "\n",
    "# #_____TRAIN MODEL\n",
    "# train(candlesDataBase_BigQuery_Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATOM/USDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 48856/48856 [00:03<00:00, 15443.34rows/s]\n",
      "/tmp/ipykernel_188142/625403997.py:462: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  candlesDataBase_BigQuery[j+\"_1\"]=candlesDataBase_BigQuery[j].shift(1)\n",
      "/tmp/ipykernel_188142/625403997.py:463: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  candlesDataBase_BigQuery[j+\"_2\"]=candlesDataBase_BigQuery[j].shift(2)\n",
      "/tmp/ipykernel_188142/625403997.py:464: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  candlesDataBase_BigQuery[j+\"_3\"]=candlesDataBase_BigQuery[j].shift(3)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'holahola' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_188142/4137464525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                     \u001b[0moutput_tradingBook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MY_UTILITY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tradingBook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"MY_UTILITY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutput_tradingBook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NET_UTILITY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mholahola\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'holahola' is not defined"
     ]
    }
   ],
   "source": [
    "marketDict=readPickleVariable(\"marketDict\")\n",
    "iteration=list(marketDict.keys())\n",
    "\n",
    "#_____UPDATE CONFIG PREDICTION\n",
    "updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "fractal=\"15\"\n",
    "\n",
    "for itt in iteration:\n",
    "    print(itt)\n",
    "    \n",
    "    #market_model=\"LUNA/USDT\"\n",
    "    market_model=itt\n",
    "\n",
    "    #_____CARGAR BASE DE DATOS\n",
    "    # myData = binance.fetch_ohlcv(market_model,fractal+\"m\",limit=1000)\n",
    "    # candlesDataBase_BigQuery =pd.DataFrame(columns=[\"ID\",\"TIME\",\"MARKET\",\"OPEN\",\"HIGH\",\"LOW\",\"CLOSE\",\"VOLUME\"])\n",
    "\n",
    "    market_model=market_model.replace(\"/\",\"_\")+\"_1M\"\n",
    "    candlesDataBase_BigQuery=downloadDataBaseBigQuery(market_model,time=fractal,rows=int(get_config(\"PARAMETERS\",\"ROWS_DOWNLOAD\")))\n",
    "\n",
    "    # for i in range(0,len(myData)):\n",
    "    #     candlesDataBase_BigQuery.at[i,\"ID\"]=myData[i][0]\n",
    "    #     candlesDataBase_BigQuery.at[i,\"TIME\"]=datetime.fromtimestamp(myData[i][0]/1000.0)\n",
    "    #     candlesDataBase_BigQuery.at[i,\"MARKET\"]=market_model\n",
    "    #     candlesDataBase_BigQuery.at[i,\"OPEN\"]=myData[i][1]\n",
    "    #     candlesDataBase_BigQuery.at[i,\"HIGH\"]=myData[i][2]\n",
    "    #     candlesDataBase_BigQuery.at[i,\"LOW\"]=myData[i][3]\n",
    "    #     candlesDataBase_BigQuery.at[i,\"CLOSE\"]=myData[i][4]\n",
    "    #     candlesDataBase_BigQuery.at[i,\"VOLUME\"]=myData[i][5]\n",
    "\n",
    "    #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "    backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "    #_____CREAR VARIABLES INPUT\n",
    "    candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "    #_____RSI\n",
    "    myRSI=list(candlesDataBase_BigQuery.RSI.values)\n",
    "    myMACD=list(candlesDataBase_BigQuery.MACD.values)\n",
    "    # myMACD_SHORT=list(candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_SHORT\")].values)\n",
    "    # myMACD_LONG=list(candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_LONG\")].values)\n",
    "    myMACD_TRIGGER=list(candlesDataBase_BigQuery.MACD_TRIGGER.values)\n",
    "\n",
    "    #_____CREAR VARIABLE DE RESPUESTA\n",
    "    candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "    candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "    candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    #_____ESCALAR VARIABLES\n",
    "    candlesDataBase_BigQuery=scaleTestVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "    #_____PREPARACIÓN PARA PREDICCIÓN\n",
    "    continuous_variables=readPickleVariable(\"variableList\")\n",
    "    train_model=readPickleVariable(\"train_model\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        #_____DATOS\n",
    "        X_test=candlesDataBase_BigQuery.copy()\n",
    "        X_test=X_test[continuous_variables]\n",
    "        X_test=X_test.values\n",
    "\n",
    "        #_____PREDICTION\n",
    "        candlesDataBase_BigQuery[\"PRED\"]=train_model.predict(np.array(X_test))\n",
    "\n",
    "        #_____CORRECT PREDICT\n",
    "        PERIODS_EVALUATION_PAST=15\n",
    "        MAX_PERIODS_EVALUATION_QUANTILE=0.95\n",
    "        PERIODS_LINEAREG_BEFORE=10\n",
    "        candlesDataBase_BigQuery=correctPredict(candlesDataBase_BigQuery,PERIODS_EVALUATION_PAST,MAX_PERIODS_EVALUATION_QUANTILE,PERIODS_LINEAREG_BEFORE)\n",
    "\n",
    "        #_____DATE\n",
    "        past_date = datetime.now()\n",
    "\n",
    "        #_____BACKTESTING\n",
    "        fractal=\"15\"\n",
    "        market_model=market_model.replace(\"/\",\"_\")\n",
    "        test_sample=candlesDataBase_BigQuery.copy()\n",
    "        backtesting(test_sample)\n",
    "\n",
    "        #_____DATE\n",
    "        future_date = datetime.now()\n",
    "        difference = (future_date - past_date)\n",
    "        total_seconds = difference.total_seconds()\n",
    "        total_seconds\n",
    "\n",
    "        hola=1\n",
    "\n",
    "    except:\n",
    "        hola =0\n",
    "\n",
    "    if hola==1:\n",
    "\n",
    "        if len(output_tradingBook)>0:\n",
    "            for i in range(0,len(output_tradingBook)):\n",
    "                if i == 0:\n",
    "                    output_tradingBook.at[i,\"MY_UTILITY\"]=100*(1+output_tradingBook.at[i,\"NET_UTILITY\"])\n",
    "                else:\n",
    "                    output_tradingBook.at[i,\"MY_UTILITY\"]=output_tradingBook.at[i-1,\"MY_UTILITY\"]*(1+output_tradingBook.at[i,\"NET_UTILITY\"])\n",
    "            holahola\n",
    "\n",
    "    clear_output()\n",
    "    \n",
    "print(itt,len(output_tradingBook.loc[output_tradingBook.STOP_LOSS==0])/len(output_tradingBook),len(output_tradingBook.loc[output_tradingBook.STOP_LOSS==1])/len(output_tradingBook))\n",
    "output_tradingBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATOM/USDT 0.76 0.24\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE_BUY</th>\n",
       "      <th>DATE_SELL</th>\n",
       "      <th>NET_BUY_PRICE</th>\n",
       "      <th>NET_SELL_PRICE</th>\n",
       "      <th>STOP_LOSS</th>\n",
       "      <th>NET_UTILITY</th>\n",
       "      <th>MY_UTILITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-11 04:15:00</td>\n",
       "      <td>2022-01-11 04:30:00</td>\n",
       "      <td>38.25822</td>\n",
       "      <td>38.64132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>101.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-11 06:30:00</td>\n",
       "      <td>2022-01-11 10:15:00</td>\n",
       "      <td>38.77874</td>\n",
       "      <td>37.4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.036518</td>\n",
       "      <td>97.312939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-11 18:15:00</td>\n",
       "      <td>2022-01-11 20:30:00</td>\n",
       "      <td>38.38835</td>\n",
       "      <td>38.81115</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011014</td>\n",
       "      <td>98.384720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-22 11:45:00</td>\n",
       "      <td>2022-01-22 12:15:00</td>\n",
       "      <td>29.70968</td>\n",
       "      <td>30.28968</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019522</td>\n",
       "      <td>100.305412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-22 17:15:00</td>\n",
       "      <td>2022-01-22 18:30:00</td>\n",
       "      <td>29.20918</td>\n",
       "      <td>28.29</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.032437</td>\n",
       "      <td>97.051765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-01-22 20:30:00</td>\n",
       "      <td>2022-01-22 20:45:00</td>\n",
       "      <td>29.03901</td>\n",
       "      <td>29.53044</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016923</td>\n",
       "      <td>98.694181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-01-22 22:30:00</td>\n",
       "      <td>2022-01-22 23:15:00</td>\n",
       "      <td>28.88886</td>\n",
       "      <td>29.41056</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>100.476486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-01-23 05:15:00</td>\n",
       "      <td>2022-01-23 05:30:00</td>\n",
       "      <td>30.28025</td>\n",
       "      <td>30.65931</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012518</td>\n",
       "      <td>101.734290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-01-23 18:15:00</td>\n",
       "      <td>2022-01-23 19:30:00</td>\n",
       "      <td>32.80277</td>\n",
       "      <td>31.54</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.039457</td>\n",
       "      <td>97.720126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-01-23 21:15:00</td>\n",
       "      <td>2022-01-23 21:30:00</td>\n",
       "      <td>32.29226</td>\n",
       "      <td>32.75721</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014398</td>\n",
       "      <td>99.127118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-01-23 22:45:00</td>\n",
       "      <td>2022-01-23 23:00:00</td>\n",
       "      <td>32.51248</td>\n",
       "      <td>33.81615</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040098</td>\n",
       "      <td>103.101871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-01-24 01:00:00</td>\n",
       "      <td>2022-01-24 04:45:00</td>\n",
       "      <td>33.9339</td>\n",
       "      <td>32.81</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>99.587421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-01-24 12:45:00</td>\n",
       "      <td>2022-01-24 13:15:00</td>\n",
       "      <td>30.46043</td>\n",
       "      <td>30.77919</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010465</td>\n",
       "      <td>100.629576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-01-24 17:15:00</td>\n",
       "      <td>2022-01-24 17:30:00</td>\n",
       "      <td>33.85382</td>\n",
       "      <td>34.98498</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>103.991919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-01-25 04:45:00</td>\n",
       "      <td>2022-01-25 08:30:00</td>\n",
       "      <td>35.8358</td>\n",
       "      <td>36.40356</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015843</td>\n",
       "      <td>105.639502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-01-25 10:45:00</td>\n",
       "      <td>2022-01-25 16:30:00</td>\n",
       "      <td>36.1361</td>\n",
       "      <td>36.67329</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>107.209912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-01-25 18:30:00</td>\n",
       "      <td>2022-01-25 20:00:00</td>\n",
       "      <td>36.47644</td>\n",
       "      <td>35.38</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.031029</td>\n",
       "      <td>103.883318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-26 00:15:00</td>\n",
       "      <td>2022-01-26 01:00:00</td>\n",
       "      <td>35.29526</td>\n",
       "      <td>33.96</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.038793</td>\n",
       "      <td>99.853340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-01-26 03:15:00</td>\n",
       "      <td>2022-01-26 04:45:00</td>\n",
       "      <td>34.64461</td>\n",
       "      <td>35.22474</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016745</td>\n",
       "      <td>101.525401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-01-27 06:15:00</td>\n",
       "      <td>2022-01-27 06:45:00</td>\n",
       "      <td>30.31028</td>\n",
       "      <td>30.69927</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012834</td>\n",
       "      <td>102.828338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-01-27 23:30:00</td>\n",
       "      <td>2022-01-27 23:45:00</td>\n",
       "      <td>29.74972</td>\n",
       "      <td>30.04992</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010091</td>\n",
       "      <td>103.865963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-01-28 00:45:00</td>\n",
       "      <td>2022-01-28 01:00:00</td>\n",
       "      <td>29.76974</td>\n",
       "      <td>30.12984</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012096</td>\n",
       "      <td>105.122344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-01-28 04:15:00</td>\n",
       "      <td>2022-01-28 05:30:00</td>\n",
       "      <td>29.07905</td>\n",
       "      <td>29.44053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>106.429114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-02-03 12:15:00</td>\n",
       "      <td>2022-02-03 12:45:00</td>\n",
       "      <td>26.15613</td>\n",
       "      <td>26.61336</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017481</td>\n",
       "      <td>108.289579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-02-03 16:45:00</td>\n",
       "      <td>2022-02-03 17:30:00</td>\n",
       "      <td>28.09807</td>\n",
       "      <td>28.40157</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>109.459264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DATE_BUY            DATE_SELL NET_BUY_PRICE NET_SELL_PRICE  \\\n",
       "0   2022-01-11 04:15:00  2022-01-11 04:30:00      38.25822       38.64132   \n",
       "1   2022-01-11 06:30:00  2022-01-11 10:15:00      38.77874           37.4   \n",
       "2   2022-01-11 18:15:00  2022-01-11 20:30:00      38.38835       38.81115   \n",
       "3   2022-01-22 11:45:00  2022-01-22 12:15:00      29.70968       30.28968   \n",
       "4   2022-01-22 17:15:00  2022-01-22 18:30:00      29.20918          28.29   \n",
       "5   2022-01-22 20:30:00  2022-01-22 20:45:00      29.03901       29.53044   \n",
       "6   2022-01-22 22:30:00  2022-01-22 23:15:00      28.88886       29.41056   \n",
       "7   2022-01-23 05:15:00  2022-01-23 05:30:00      30.28025       30.65931   \n",
       "8   2022-01-23 18:15:00  2022-01-23 19:30:00      32.80277          31.54   \n",
       "9   2022-01-23 21:15:00  2022-01-23 21:30:00      32.29226       32.75721   \n",
       "10  2022-01-23 22:45:00  2022-01-23 23:00:00      32.51248       33.81615   \n",
       "11  2022-01-24 01:00:00  2022-01-24 04:45:00       33.9339          32.81   \n",
       "12  2022-01-24 12:45:00  2022-01-24 13:15:00      30.46043       30.77919   \n",
       "13  2022-01-24 17:15:00  2022-01-24 17:30:00      33.85382       34.98498   \n",
       "14  2022-01-25 04:45:00  2022-01-25 08:30:00       35.8358       36.40356   \n",
       "15  2022-01-25 10:45:00  2022-01-25 16:30:00       36.1361       36.67329   \n",
       "16  2022-01-25 18:30:00  2022-01-25 20:00:00      36.47644          35.38   \n",
       "17  2022-01-26 00:15:00  2022-01-26 01:00:00      35.29526          33.96   \n",
       "18  2022-01-26 03:15:00  2022-01-26 04:45:00      34.64461       35.22474   \n",
       "19  2022-01-27 06:15:00  2022-01-27 06:45:00      30.31028       30.69927   \n",
       "20  2022-01-27 23:30:00  2022-01-27 23:45:00      29.74972       30.04992   \n",
       "21  2022-01-28 00:45:00  2022-01-28 01:00:00      29.76974       30.12984   \n",
       "22  2022-01-28 04:15:00  2022-01-28 05:30:00      29.07905       29.44053   \n",
       "23  2022-02-03 12:15:00  2022-02-03 12:45:00      26.15613       26.61336   \n",
       "24  2022-02-03 16:45:00  2022-02-03 17:30:00      28.09807       28.40157   \n",
       "\n",
       "   STOP_LOSS NET_UTILITY  MY_UTILITY  \n",
       "0          0    0.010014  101.001353  \n",
       "1          1   -0.036518   97.312939  \n",
       "2          0    0.011014   98.384720  \n",
       "3          0    0.019522  100.305412  \n",
       "4          1   -0.032437   97.051765  \n",
       "5          0    0.016923   98.694181  \n",
       "6          0    0.018059  100.476486  \n",
       "7          0    0.012518  101.734290  \n",
       "8          1   -0.039457   97.720126  \n",
       "9          0    0.014398   99.127118  \n",
       "10         0    0.040098  103.101871  \n",
       "11         1   -0.034087   99.587421  \n",
       "12         0    0.010465  100.629576  \n",
       "13         0    0.033413  103.991919  \n",
       "14         0    0.015843  105.639502  \n",
       "15         0    0.014866  107.209912  \n",
       "16         1   -0.031029  103.883318  \n",
       "17         1   -0.038793   99.853340  \n",
       "18         0    0.016745  101.525401  \n",
       "19         0    0.012834  102.828338  \n",
       "20         0    0.010091  103.865963  \n",
       "21         0    0.012096  105.122344  \n",
       "22         0    0.012431  106.429114  \n",
       "23         0    0.017481  108.289579  \n",
       "24         0    0.010801  109.459264  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(itt,len(output_tradingBook.loc[output_tradingBook.STOP_LOSS==0])/len(output_tradingBook),len(output_tradingBook.loc[output_tradingBook.STOP_LOSS==1])/len(output_tradingBook))\n",
    "# output_tradingBook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____PARÁMETROS\n",
    "# candlesDataBase_Graph=candlesDataBase_BigQuery.copy()\n",
    "# predType=\"BUY\"\n",
    "\n",
    "# #_____AGREGAR VARIABLE CLOSE\n",
    "# for i in range(0,len(candlesDataBase_Graph)):\n",
    "#     candlesDataBase_Graph.at[i,\"CLOSE\"]=backtestingDataFrame.loc[backtestingDataFrame.TIME==candlesDataBase_Graph.at[i,\"TIME\"]][\"CLOSE\"].values[0]\n",
    "\n",
    "# #_____AGREGAR COLORES\n",
    "# candlesDataBase_Color=candlesDataBase_Graph.copy()\n",
    "# candlesDataBase_Color=candlesDataBase_Color[[predType]]\n",
    "# candlesDataBase_Color=np.where(candlesDataBase_Color[predType] ==\"SI\", \"red\", \"blue\")\n",
    "\n",
    "# #_____DAR TEXTO A EJES\n",
    "# x_buy = candlesDataBase_Graph[[\"TIME\"]].values\n",
    "# y_buy = candlesDataBase_Graph[[\"CLOSE\"]].values\n",
    "# color_buy = candlesDataBase_Color\n",
    "\n",
    "# #_____RESETEAR\n",
    "# fig_buy = None\n",
    "\n",
    "# #_____IMPRIMIR GRÁFICA\n",
    "# fig_buy = plt.figure(1, figsize=(20,10))\n",
    "# ax_buy  = fig_buy.add_subplot(111)\n",
    "# plot_colourline(x_buy,y_buy,color_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #_____PARÁMETROS\n",
    "# candlesDataBase_Graph=candlesDataBase_BigQuery.copy()\n",
    "# predType=\"PRED\"\n",
    "\n",
    "# #_____AGREGAR VARIABLE CLOSE\n",
    "# for i in range(0,len(candlesDataBase_Graph)):\n",
    "#     candlesDataBase_Graph.at[i,\"CLOSE\"]=backtestingDataFrame.loc[backtestingDataFrame.TIME==candlesDataBase_Graph.at[i,\"TIME\"]][\"CLOSE\"].values[0]\n",
    "\n",
    "# #_____ZOOM\n",
    "# # candlesDataBase_Graph=candlesDataBase_Graph[len(candlesDataBase_Graph)-400:len(candlesDataBase_Graph)-300]\n",
    "# # candlesDataBase_Graph=candlesDataBase_Graph[len(candlesDataBase_Graph)-400:]\n",
    "# # candlesDataBase_Graph.reset_index(inplace=True)\n",
    "\n",
    "# #_____AGREGAR COLORES\n",
    "# candlesDataBase_Color=candlesDataBase_Graph.copy()\n",
    "# candlesDataBase_Color=candlesDataBase_Color[[predType]]\n",
    "# candlesDataBase_Color=np.where(candlesDataBase_Color[predType] ==\"SI\", \"red\", \"blue\")\n",
    "\n",
    "# #_____DAR TEXTO A EJES\n",
    "# x_buy = candlesDataBase_Graph[[\"TIME\"]].values\n",
    "# y_buy = candlesDataBase_Graph[[\"CLOSE\"]].values\n",
    "# color_buy = candlesDataBase_Color\n",
    "\n",
    "# #_____RESETEAR\n",
    "# fig_buy = None\n",
    "\n",
    "# #_____IMPRIMIR GRÁFICA\n",
    "# fig_buy = plt.figure(1, figsize=(20,10))\n",
    "# ax_buy  = fig_buy.add_subplot(111)\n",
    "# plot_colourline(x_buy,y_buy,color_buy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKTESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #_____DATE\n",
    "# past_date = datetime.now()\n",
    "\n",
    "# #_____BACKTESTING\n",
    "# fractal=\"15\"\n",
    "# market_model=market_model.replace(\"/\",\"_\")\n",
    "# test_sample=candlesDataBase_BigQuery.copy()\n",
    "# backtesting(test_sample)\n",
    "\n",
    "# #_____DATE\n",
    "# future_date = datetime.now()\n",
    "# difference = (future_date - past_date)\n",
    "# total_seconds = difference.total_seconds()\n",
    "# total_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN - TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #_____LOOP INFINITO\n",
    "# while True:\n",
    "    \n",
    "#     #_____CONEXIÓN BINANCE\n",
    "#     binance = ccxt.binance()\n",
    "    \n",
    "#     #_____MARKET LIST DE MERCADOS POTENCIALES\n",
    "#     marketsIterationList=marketList()\n",
    "#     random.shuffle(marketsIterationList)\n",
    "\n",
    "#     #_____UPDATE CONFIG PREDICTION\n",
    "#     updateConfig(\"semiManual.ini\")\n",
    "    \n",
    "#     #_____MARKETDICT\n",
    "#     marketDict=readPickleVariable(\"marketDict\")\n",
    "    \n",
    "#     #_____FOR LOOP PARA ITERACIÓN DE ALERTA DE MERCADOS\n",
    "#     for market in marketsIterationList:\n",
    "        \n",
    "#         #_____PRINT WITNESS\n",
    "#         clear_output()\n",
    "#         print(market+\"/USDT\",str(datetime.now()-timedelta(hours=5)))\n",
    "\n",
    "#         #_____PARÁMETROS\n",
    "#         fractal=\"15\"\n",
    "#         market_model=market+\"/USDT\"\n",
    "        \n",
    "#         #_____SI MERCADO ESTÁ DENTRO DE MERCADOS EVALUADOS\n",
    "#         if market_model in marketDict.keys():\n",
    "        \n",
    "#             #_____UPDATE CONFIG PREDICTION\n",
    "#             updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "#             #_____CARGAR BASE DE DATOS\n",
    "#             myData = binance.fetch_ohlcv(market_model,fractal+\"m\",limit=1000)\n",
    "#             candlesDataBase_BigQuery =pd.DataFrame(columns=[\"ID\",\"TIME\",\"MARKET\",\"OPEN\",\"HIGH\",\"LOW\",\"CLOSE\",\"VOLUME\"])\n",
    "\n",
    "#             for i in range(0,len(myData)):\n",
    "#                 candlesDataBase_BigQuery.at[i,\"ID\"]=myData[i][0]\n",
    "#                 candlesDataBase_BigQuery.at[i,\"TIME\"]=datetime.fromtimestamp(myData[i][0]/1000.0)\n",
    "#                 candlesDataBase_BigQuery.at[i,\"MARKET\"]=market_model\n",
    "#                 candlesDataBase_BigQuery.at[i,\"OPEN\"]=myData[i][1]\n",
    "#                 candlesDataBase_BigQuery.at[i,\"HIGH\"]=myData[i][2]\n",
    "#                 candlesDataBase_BigQuery.at[i,\"LOW\"]=myData[i][3]\n",
    "#                 candlesDataBase_BigQuery.at[i,\"CLOSE\"]=myData[i][4]\n",
    "#                 candlesDataBase_BigQuery.at[i,\"VOLUME\"]=myData[i][5]\n",
    "\n",
    "#             #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "#             backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "#             #_____CREAR VARIABLES INPUT\n",
    "#             candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "#             #_____RSI\n",
    "#             myRSI=list(candlesDataBase_BigQuery.RSI.values)\n",
    "#             myMACD=list(candlesDataBase_BigQuery.MACD.values)\n",
    "#             # myMACD_SHORT=list(candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_SHORT\")].values)\n",
    "#             # myMACD_LONG=list(candlesDataBase_BigQuery[\"MACD_\"+get_config(\"PARAMETERS\",\"MACD_LONG\")].values)\n",
    "#             myMACD_TRIGGER=list(candlesDataBase_BigQuery.MACD_TRIGGER.values)\n",
    "\n",
    "#             #_____CREAR VARIABLE DE RESPUESTA\n",
    "#             candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "#             candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "#             candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#             #_____ESCALAR VARIABLES\n",
    "#             candlesDataBase_BigQuery=scaleTestVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "#             #_____PREPARACIÓN PARA PREDICCIÓN\n",
    "#             continuous_variables=readPickleVariable(\"variableList\")\n",
    "#             train_model=readPickleVariable(\"train_model\")\n",
    "\n",
    "#             #_____DATOS\n",
    "#             X_test=candlesDataBase_BigQuery.copy()\n",
    "#             X_test=X_test[continuous_variables]\n",
    "#             X_test=X_test.values\n",
    "\n",
    "#             #_____PREDICTION\n",
    "#             candlesDataBase_BigQuery[\"PRED\"]=train_model.predict(np.array(X_test))\n",
    "\n",
    "#             #_____CORRECT PREDICT\n",
    "#             PERIODS_EVALUATION_PAST=15\n",
    "#             MAX_PERIODS_EVALUATION_QUANTILE=0.95\n",
    "#             PERIODS_LINEAREG_BEFORE=10\n",
    "#             candlesDataBase_BigQuery=correctPredict(candlesDataBase_BigQuery,PERIODS_EVALUATION_PAST,MAX_PERIODS_EVALUATION_QUANTILE,PERIODS_LINEAREG_BEFORE)\n",
    "\n",
    "#             #_____TWITTEAR CUANDO SEA EL CASO\n",
    "#             if candlesDataBase_BigQuery.at[len(candlesDataBase_BigQuery)-1,\"PRED\"]==\"SI\":\n",
    "#                 while True:\n",
    "#                     try:\n",
    "#                         oauth = OAuth()\n",
    "#                         api = tw.API(oauth)\n",
    "#                         tweet=api.update_status(\"¡TRADING ALERT! -> {} -> (SURF)\".format(market))\n",
    "#                         break\n",
    "#                     except:\n",
    "#                         time.sleep(10)\n",
    "\n",
    "#             #_____SLEEP\n",
    "#             time.sleep(30)\n",
    "#             clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN - PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____CCXT\n",
    "# binance = ccxt.binance()\n",
    "\n",
    "# #_____GOOGLE CLOUD CONECTION\n",
    "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/ubuntu/SurfNet/predictionModels/PREDICTION/bigQueryAccess.json\"\n",
    "\n",
    "# while True:\n",
    "\n",
    "#     #_____NUEVA LISTA DE MERCADOS\n",
    "#     marketDict=readPickleVariable(\"marketDict\")\n",
    "#     mercados=list(marketDict.keys())\n",
    "#     random.shuffle(mercados)\n",
    "    \n",
    "#     for market_model in mercados:\n",
    "        \n",
    "#         #_____UPDATE CONFIG PREDICTION\n",
    "#         updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "#         #_____PARAMETERS\n",
    "#         fractal=\"15\"\n",
    "        \n",
    "#         #_____PRINT\n",
    "#         clear_output()\n",
    "#         print(market_model,str(datetime.now()))\n",
    "\n",
    "#         #_____CARGAR BASE DE DATOS\n",
    "#         myData = binance.fetch_ohlcv(market_model,fractal+\"m\",limit=1000)\n",
    "#         candlesDataBase_BigQuery =pd.DataFrame(columns=[\"ID\",\"TIME\",\"MARKET\",\"OPEN\",\"HIGH\",\"LOW\",\"CLOSE\",\"VOLUME\"])\n",
    "\n",
    "#         for i in range(0,len(myData)):\n",
    "#             candlesDataBase_BigQuery.at[i,\"ID\"]=myData[i][0]\n",
    "#             candlesDataBase_BigQuery.at[i,\"TIME\"]=datetime.fromtimestamp(myData[i][0]/1000.0)\n",
    "#             candlesDataBase_BigQuery.at[i,\"MARKET\"]=market_model\n",
    "#             candlesDataBase_BigQuery.at[i,\"OPEN\"]=myData[i][1]\n",
    "#             candlesDataBase_BigQuery.at[i,\"HIGH\"]=myData[i][2]\n",
    "#             candlesDataBase_BigQuery.at[i,\"LOW\"]=myData[i][3]\n",
    "#             candlesDataBase_BigQuery.at[i,\"CLOSE\"]=myData[i][4]\n",
    "#             candlesDataBase_BigQuery.at[i,\"VOLUME\"]=myData[i][5]\n",
    "\n",
    "#         #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "#         backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "#         #_____CREAR VARIABLES INPUT\n",
    "#         candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "#         #_____CREAR VARIABLE DE RESPUESTA\n",
    "#         candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "#         candlesDataBase_BigQuery.dropna(inplace=True)\n",
    "#         candlesDataBase_BigQuery.reset_index(inplace=True,drop=True)\n",
    "        \n",
    "#         #_____RSI\n",
    "#         myRSI=list(candlesDataBase_BigQuery.RSI.values)\n",
    "\n",
    "#         #_____ESCALAR VARIABLES\n",
    "#         candlesDataBase_BigQuery=scaleTestVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "#         #_____PREPARACIÓN PARA PREDICCIÓN\n",
    "#         continuous_variables=readPickleVariable(\"variableList\")\n",
    "#         train_model=readPickleVariable(\"train_model\")\n",
    "\n",
    "#         #_____DATOS\n",
    "#         X_test=candlesDataBase_BigQuery.copy()\n",
    "#         X_test=X_test[continuous_variables]\n",
    "#         X_test=X_test.values\n",
    "\n",
    "#         #_____PREDICTION\n",
    "#         candlesDataBase_BigQuery[\"PRED\"]=train_model.predict(np.array(X_test))\n",
    "\n",
    "#         #_____BACKTESTING\n",
    "#         market_model=market_model.replace(\"/\",\"_\")\n",
    "#         test_sample=candlesDataBase_BigQuery.copy()\n",
    "#         backtesting(test_sample)\n",
    "        \n",
    "#         #_____SLEEP\n",
    "#         time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN - PAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #_____CCXT\n",
    "# binance = ccxt.binance()\n",
    "\n",
    "# #_____LOOP INFINITO\n",
    "# while True:\n",
    "    \n",
    "#     try:\n",
    "    \n",
    "#         #_____GOOGLE CLOUD CONECTION\n",
    "#         os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/ubuntu/SurfNet/predictionModels/PREDICTION/bigQueryAccess.json\"\n",
    "\n",
    "#         #_____ACTUALIZAR ARCHIVO CONFIG PARA MERCADOS\n",
    "#         updateConfig('/home/ubuntu/SurfNet/candleDataBases/candleDataBases.ini')\n",
    "\n",
    "#         #_____CREAR LISTA DE MERCADOS A EVALUAR\n",
    "#         mercados_Portafolio_Bruto=[]\n",
    "#         mercados=ast.literal_eval(get_config(\"DATABASES\",\"MARKETS\"))\n",
    "\n",
    "#         #_____LISTA VOLUMEN MERCADOS\n",
    "#         marketZeroVolumes_Prop=[]\n",
    "#         marketAverVolumes_List=[]\n",
    "\n",
    "#         #_____LISTA SAFE DE MERCADOS\n",
    "#         safeList=[]\n",
    "\n",
    "#         #_____RECORRER MERCADOS\n",
    "#         for mercado in mercados:\n",
    "\n",
    "#             #_____DESCARGAR BASE DE DATOS OLCV\n",
    "#             candlesDataBase_Binance = binance.fetch_ohlcv(mercado+\"/USDT\",\"1m\",limit=1000)\n",
    "#             candlesDataBase_Binance = candlesDataBase_Binance[:-1]\n",
    "\n",
    "#             #_____PROPORCIÓN MERCADO MUERTO\n",
    "#             contador_cero=0\n",
    "#             contador_aver=0\n",
    "#             for i in list(range(0,len(candlesDataBase_Binance))):\n",
    "#                 if candlesDataBase_Binance[i][5] == 0:\n",
    "#                     contador_cero=contador_cero+1\n",
    "#                 contador_aver=contador_aver+candlesDataBase_Binance[i][5]\n",
    "\n",
    "#             marketZeroVolumes_Prop.append(contador_cero/len(candlesDataBase_Binance))\n",
    "#             marketAverVolumes_List.append(contador_aver/len(candlesDataBase_Binance))\n",
    "\n",
    "#             #_____ACTUALIZAR SAFE LIST\n",
    "#             if ((contador_cero/len(candlesDataBase_Binance)) == 0):\n",
    "#                 safeList.append(mercado)\n",
    "\n",
    "#             #_____SLEEP\n",
    "#             time.sleep(2)\n",
    "\n",
    "#         #_____NUEVA LISTA DE MERCADOS\n",
    "#         mercados=safeList\n",
    "\n",
    "#         #_____AGREGAR APELLIDO A MERCADO\n",
    "#         for i in mercados:\n",
    "#             mercados_Portafolio_Bruto.append(i+\"_USDT_1M\")\n",
    "\n",
    "#         #_____RECORRER LISTA DE MERCADOS SOBRE LOS CUALES REALIZAR MODELOS\n",
    "#         for market_model in mercados_Portafolio_Bruto:\n",
    "\n",
    "#             #_____UPDATE CONFIG PREDICTION\n",
    "#             updateConfig(\"predictionModels.ini\")\n",
    "\n",
    "#             #_____ITERAR SOBRE FRACTAL\n",
    "#             for fractal in ast.literal_eval(get_config(\"PARAMETERS\",\"FRACTAL\")):\n",
    "\n",
    "#                 #_____PRINT\n",
    "#                 print(market_model,fractal,datetime.now())\n",
    "\n",
    "#                 #_____CARGAR BASE DE DATOS\n",
    "#                 candlesDataBase_BigQuery=downloadDataBaseBigQuery(market_model,time=fractal,rows=int(get_config(\"PARAMETERS\",\"ROWS_DOWNLOAD\")))\n",
    "\n",
    "#                 #_____GUARDAR BASE DE DATOS PARA BACKTESTING FUTURO\n",
    "#                 backtestingDataFrame=candlesDataBase_BigQuery.copy()\n",
    "\n",
    "#                 #_____CREAR VARIABLES INPUT\n",
    "#                 candlesDataBase_BigQuery=variableCreation(candlesDataBase_BigQuery)\n",
    "\n",
    "#                 #_____CREAR VARIABLE DE RESPUESTA\n",
    "#                 candlesDataBase_BigQuery=variableBuy(candlesDataBase_BigQuery)\n",
    "\n",
    "#                 #_____ESCALAR VARIABLES\n",
    "#                 candlesDataBase_BigQuery=scaleVariables(candlesDataBase_BigQuery)\n",
    "\n",
    "#                 #_____FILTRAR VARIABLES\n",
    "#                 candlesDataBase_BigQuery=boruta(candlesDataBase_BigQuery)\n",
    "\n",
    "#                 #_____ENTRENAR MODELO\n",
    "#                 train(candlesDataBase_BigQuery,market_model)\n",
    "\n",
    "#                 #_____PREDECIR VALORES DE TESTEO\n",
    "#                 test_sample[\"PRED\"]=predictFunction()\n",
    "\n",
    "#                 #_____BACKTESTING\n",
    "#                 backtesting(test_sample)\n",
    "\n",
    "#                 #_____GRAFICAR PREDICCIONES\n",
    "#                 #graphPredictions_BUY(market_model)\n",
    "\n",
    "#                 #_____GRAFICAR PREDICCIONES\n",
    "#                 #graphPredictions_PRED(market_model)\n",
    "\n",
    "#                 #_____CLEAR OUTPUT\n",
    "#                 clear_output()\n",
    "\n",
    "#     except:\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
